{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from matplotlib import cm, pyplot as plt\n",
    "from sklearn import metrics\n",
    "from os.path import expanduser as eu\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import keras\n",
    "import os\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto(allow_soft_placement=True,gpu_options = tf.GPUOptions(allow_growth=True))\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "def load_min_model_helper(MPATH):\n",
    "    print(\"[PROGRESS] Starting load_min_model_helper()\")\n",
    "    print(\"[DEBUG] MPATH {}\".format(MPATH))\n",
    "    mfiles = os.listdir(MPATH)\n",
    "    full_mod_name = MPATH.split(\"/\")[-1]\n",
    "    mfiles = [f for f in mfiles if \"val_loss\" in f]\n",
    "    loss_lst = [float(f.split(\"val_loss:\")[1].split(\"_\")[0]) for f in mfiles]\n",
    "    min_ind = loss_lst.index(min(loss_lst))\n",
    "    min_mod_name = \"{}/{}\".format(MPATH,mfiles[min_ind])\n",
    "    if DEBUG: print(\"[DEBUG] min_mod_name {}\".format(mfiles[min_ind]))\n",
    "    return(load_model(min_mod_name))\n",
    "\n",
    "def split_data(trainvalX,trainvalY):\n",
    "    train_ratio = 0.9\n",
    "    nine_tenths_ind = int(train_ratio*trainvalX.shape[0])\n",
    "    X_train = trainvalX[0:nine_tenths_ind,:]\n",
    "    y_train = trainvalY[0:nine_tenths_ind]\n",
    "    X_valid = trainvalX[nine_tenths_ind:trainvalX.shape[0],:]\n",
    "    y_valid = trainvalY[nine_tenths_ind:trainvalX.shape[0]]\n",
    "    del trainvalX\n",
    "    gc.collect()\n",
    "    \n",
    "    # Randomize\n",
    "    random.seed(10)\n",
    "    indices = np.arange(0,X_train.shape[0])\n",
    "    random.shuffle(indices)\n",
    "    X_train = X_train[indices,:]\n",
    "    y_train = y_train[indices]\n",
    "\n",
    "    indices = np.arange(0,X_valid.shape[0])\n",
    "    random.shuffle(indices)\n",
    "    X_valid = X_valid[indices,:]\n",
    "    y_valid = y_valid[indices]\n",
    "    return(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "def create_lstm_model(hyper_dict):\n",
    "    print(\"[PROGRESS] Starting create_lstm_model()\")\n",
    "    loss_func = \"binary_crossentropy\"; lookback = 60\n",
    "    \n",
    "    # Load hyperparameters\n",
    "    node_size = hyper_dict[\"node_size\"]\n",
    "    epoch_num = hyper_dict[\"epoch_num\"]\n",
    "    num_layer = hyper_dict[\"num_layer\"]\n",
    "    opt_name  = hyper_dict[\"opt_name\"]\n",
    "    is_tune   = hyper_dict[\"is_tune\"]\n",
    "    b_size    = hyper_dict[\"b_size\"]\n",
    "    drop      = hyper_dict[\"drop\"]\n",
    "    lr        = hyper_dict[\"lr\"]\n",
    "    assert num_layer >= 1, \"Need at least one layer\"\n",
    "    \n",
    "    # Form the model name (for saving the model)\n",
    "    mod_name  = \"lstm\"\n",
    "    for i in range(0,num_layer):\n",
    "        mod_name += \"_{}n\".format(node_size)\n",
    "    mod_name += \"_{}ep_{}opt_{}lr\".format(epoch_num,opt_name,lr)\n",
    "    mod_name += \"_{}drop_{}bs\".format(drop,b_size)\n",
    "    MODDIR = PATH+\"models/\"+mod_name+\"/\"\n",
    "    if is_tune: MODDIR = PATH+\"models/tune/\"+mod_name+\"/\"    \n",
    "    if not os.path.exists(MODDIR): os.makedirs(MODDIR)\n",
    "        \n",
    "    # Set opt based on opt_name\n",
    "    if hyper_dict[\"opt_name\"] is \"rmsprop\":\n",
    "        opt = keras.optimizers.RMSprop(lr)\n",
    "    elif hyper_dict[\"opt_name\"] is \"sgd\":\n",
    "        opt = keras.optimizers.SGD(lr)\n",
    "    elif hyper_dict[\"opt_name\"] is \"adam\":\n",
    "        opt = keras.optimizers.Adam(lr)\n",
    "    \n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    # If one layer, no need to return sequences\n",
    "    if num_layer == 1:\n",
    "        model.add(LSTM(node_size, recurrent_dropout=drop, input_shape=(lookback,1)))\n",
    "    else: # More than one layer\n",
    "        model.add(LSTM(node_size, recurrent_dropout=drop, return_sequences=True, input_shape=(lookback,1)))\n",
    "    \n",
    "        for i in range(1,num_layer): \n",
    "            if i == num_layer-1: # Don't return sequences at last hidden layer\n",
    "                model.add(LSTM(node_size, recurrent_dropout=drop, dropout=drop))\n",
    "            else:\n",
    "                model.add(LSTM(node_size, return_sequences=True, recurrent_dropout=drop, dropout=drop))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return(model)\n",
    "    \n",
    "def train_lstm_model(RESDIR,trainvalX,trainvalY,data_type,\n",
    "                    label_type,hosp_data,hyper_dict):\n",
    "    \n",
    "    ###### Split data ######\n",
    "    X_train, y_train, X_valid, y_valid = split_data(trainvalX,trainvalY)\n",
    "    \n",
    "    ###### Form model ######\n",
    "    model = create_lstm_model(hyper_dict)\n",
    "    \n",
    "    # Train and Save\n",
    "    with open(MODDIR+\"loss.txt\", \"w\") as f:\n",
    "        f.write(\"{}\\t{}\\t{}\\t{}\\n\".format(\"i\", \"train_loss\", \"val_loss\", \"epoch_time\"))\n",
    "    diffs = []; best_loss_so_far = float(\"inf\")\n",
    "    start_time = time.time(); per_iter_size = 300000\n",
    "    np.random.seed(10)\n",
    "    for i in range(0,epoch_num):\n",
    "        if per_iter_size < X_train.shape[0]:\n",
    "            per_iter_size = X_train.shape[0]\n",
    "        inds = np.random.choice(X_train.shape[0],per_iter_size,replace=False)\n",
    "        curr_x = X_train[inds,]; curr_y = y_train[inds,]\n",
    "        history = model.fit(curr_x, curr_y, epochs=1, batch_size=1000, \n",
    "                            validation_data=(X_valid,y_valid))\n",
    "\n",
    "        # Save details about training\n",
    "        train_loss = history.history['loss'][0]\n",
    "        val_loss = history.history['val_loss'][0]\n",
    "        epoch_time = time.time() - start_time\n",
    "        with open(MODDIR+\"loss.txt\", \"a\") as f:\n",
    "            f.write(\"{}\\t{}\\t{}\\t{}\\n\".format(i, train_loss, val_loss, epoch_time))\n",
    "\n",
    "        # Save model each iteration\n",
    "        model.save(\"{}val_loss:{}_epoch:{}_{}.h5\".format(MODDIR,val_loss,i,mod_name))\n",
    "    return(MODDIR)\n",
    "\n",
    "def load_model_and_test(RESDIR,MODDIR,X_test,y_test,data_type,hosp_data):\n",
    "    model = load_min_model_helper(MODDIR)\n",
    "    save_path = RESDIR+\"hosp{}_data/{}/\".format(hosp_data,data_type)\n",
    "    if not os.path.exists(save_path): os.makedirs(save_path)\n",
    "    print(\"[DEBUG] Loading model from {}\".format(save_path))\n",
    "    ypred = model.predict(X_test)\n",
    "    np.save(save_path+\"ypred.npy\",ypred)\n",
    "    np.save(save_path+\"y_test.npy\",y_test)\n",
    "    auc = metrics.average_precision_score(y_test, ypred)\n",
    "    np.random.seed(231)\n",
    "    auc_lst = []\n",
    "    roc_auc_lst = []\n",
    "    for i in range(0,100):\n",
    "        inds = np.random.choice(X_test.shape[0], X_test.shape[0], replace=True)\n",
    "        auc = metrics.average_precision_score(y_test[inds], ypred[inds])\n",
    "        auc_lst.append(auc)\n",
    "        roc_auc = metrics.roc_auc_score(y_test[inds], ypred[inds])\n",
    "        roc_auc_lst.append(roc_auc)\n",
    "    auc_lst = np.array(auc_lst)\n",
    "    roc_auc_lst = np.array(roc_auc_lst)\n",
    "    print(\"[DEBUG] auc_lst.mean(): {}\".format(auc_lst.mean()))\n",
    "    print(\"[DEBUG] roc_auc_lst.mean(): {}\".format(roc_auc_lst.mean()))\n",
    "\n",
    "    SP = RESDIR+\"hosp{}_data/\".format(hosp_data)\n",
    "    f = open('{}conf_int_hospdata{}_prauc.txt'.format(SP,hosp_data),'a')\n",
    "    f.write(\"{}, {}+-{}\\n\".format(data_type,auc_lst.mean().round(4),2*np.std(auc_lst).round(4)))\n",
    "    f.close()\n",
    "    f = open('{}conf_int_hospdata{}_rocauc.txt'.format(SP,hosp_data),'a')\n",
    "    f.write(\"{}, {}+-{}\\n\".format(data_type,roc_auc_lst.mean().round(4),2*np.std(roc_auc_lst).round(4)))\n",
    "    f.close()\n",
    "    np.save(\"{}auc_lst\".format(save_path,data_type), auc_lst)\n",
    "    np.save(\"{}roc_auc_lst\".format(save_path,data_type), roc_auc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/projects/leelab2/hughchen/RELIC/repr_learning/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Starting create_lstm_model()\n"
     ]
    }
   ],
   "source": [
    "hyper_dict = {\"is_tune\":True,\"opt_name\":\"rmsprop\",\"lr\":0.001,\"drop\":0.5,\n",
    "              \"b_size\":1000,\"epoch_num\":50,\"node_size\":200, \"num_layer\":3}\n",
    "model = create_lstm_model(hyper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 60, 200)           161600    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 60, 200)           320800    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 803,401\n",
      "Trainable params: 803,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Progress] label_type: nibpm60, eta: NA, curr_feat NIBPM\n",
      "\n",
      "[Progress] hosp_data 0\n",
      "\n",
      "[Progress] data_type raw[top15]+nonsignal\n",
      "[DEBUG] Y.shape: (1837676,)\n",
      "[DEBUG] Starting load_raw_data\n",
      "[DEBUG] DPATH /homes/gws/hughchen/phase/downstream_prediction//data/nibpm60/hospital_0/\n",
      "\n",
      "[Progress] hosp_data 1\n",
      "\n",
      "[Progress] data_type raw[top15]+nonsignal\n",
      "[DEBUG] Y.shape: (2332902,)\n",
      "[DEBUG] Starting load_raw_data\n",
      "[DEBUG] DPATH /homes/gws/hughchen/phase/downstream_prediction//data/nibpm60/hospital_1/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from xgb_setup import *\n",
    "os.nice(5)\n",
    "PATH = \"/projects/leelab2/hughchen/RELIC/repr_learning/\"\n",
    "DPATH = \"/homes/gws/hughchen/phase/downstream_prediction/\"\n",
    "RESULTPATH = PATH+\"/results/\"; MODELPATH = PATH+\"/models/\"\n",
    "lookback = 60\n",
    "DEBUG = False\n",
    "label_type_eta_currfeat_lst = [(\"nibpm60\",0.1,\"NIBPM\")]\n",
    "\n",
    "for label_type, eta, curr_feat in label_type_eta_currfeat_lst:\n",
    "    print(\"\\n[Progress] label_type: {}, eta: {}, curr_feat {}\".format(label_type, \"NA\", curr_feat))\n",
    "\n",
    "    xgb_type = \"mlp_{}_top15\".format(label_type)\n",
    "    RESDIR = '{}{}/'.format(RESULTPATH, xgb_type)\n",
    "    if not os.path.exists(RESDIR): os.makedirs(RESDIR)\n",
    "\n",
    "    for hosp_data in [0,1]:\n",
    "        print(\"\\n[Progress] hosp_data {}\".format(hosp_data))\n",
    "\n",
    "        data_type = \"raw[top15]+nonsignal\"\n",
    "\n",
    "        print(\"\\n[Progress] data_type {}\".format(data_type))\n",
    "        (trainvalX,trainvalY) = load_data(DPATH,data_type,label_type,True,\n",
    "                                          hosp_data,curr_feat,DEBUG=DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Progress] label_type: desat_bool92_5_nodesat, eta: NA, curr_feat SAO2\n",
      "\n",
      "[Progress] hosp_data 0\n",
      "\n",
      "[Progress] hosp_model 0\n",
      "\n",
      "[Progress] data_type ema[top15]+nonsignal\n",
      "[DEBUG] Y.shape: (3920564,)\n",
      "[Progress] trainvalX.shape (3920564, 117)\n",
      "[PROGRESS] Starting create_model()\n",
      "WARNING:tensorflow:From /homes/gws/hughchen/RNN/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /homes/gws/hughchen/RNN/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 15s 4us/step - loss: 2.1208 - val_loss: 0.1725\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 15s 4us/step - loss: 0.1810 - val_loss: 0.1688\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 15s 4us/step - loss: 0.1779 - val_loss: 0.1684\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 15s 4us/step - loss: 0.1776 - val_loss: 0.1682\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.1744 - val_loss: 0.1587\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 15s 4us/step - loss: 0.1506 - val_loss: 0.1268\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.1216 - val_loss: 0.1044\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.1047 - val_loss: 0.0934\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0944 - val_loss: 0.0865\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0876 - val_loss: 0.0818\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0827 - val_loss: 0.0775\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 15s 4us/step - loss: 0.0792 - val_loss: 0.0741\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 15s 4us/step - loss: 0.0766 - val_loss: 0.0720\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0744 - val_loss: 0.0698\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0725 - val_loss: 0.0686\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0708 - val_loss: 0.0668\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0687 - val_loss: 0.0657\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0672 - val_loss: 0.0642\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0660 - val_loss: 0.0634\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0649 - val_loss: 0.0627\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0641 - val_loss: 0.0622\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0633 - val_loss: 0.0613\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0627 - val_loss: 0.0606\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0622 - val_loss: 0.0604\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0617 - val_loss: 0.0602\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0612 - val_loss: 0.0598\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0608 - val_loss: 0.0595\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 13s 4us/step - loss: 0.0604 - val_loss: 0.0592\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0601 - val_loss: 0.0586\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 15s 4us/step - loss: 0.0597 - val_loss: 0.0591\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0594 - val_loss: 0.0583\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0592 - val_loss: 0.0579\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0589 - val_loss: 0.0575\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 15s 4us/step - loss: 0.0587 - val_loss: 0.0575\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 15s 4us/step - loss: 0.0584 - val_loss: 0.0571\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 15s 4us/step - loss: 0.0582 - val_loss: 0.0569\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0580 - val_loss: 0.0567\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0578 - val_loss: 0.0566\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0576 - val_loss: 0.0565\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0575 - val_loss: 0.0565\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0573 - val_loss: 0.0562\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0571 - val_loss: 0.0561\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0569 - val_loss: 0.0560\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0568 - val_loss: 0.0558\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0566 - val_loss: 0.0557\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0565 - val_loss: 0.0557\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0564 - val_loss: 0.0557\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0562 - val_loss: 0.0554\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0561 - val_loss: 0.0555\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 14s 4us/step - loss: 0.0560 - val_loss: 0.0554\n",
      "[DEBUG] Y.shape: (493398,)\n",
      "[Progress] test1X.shape    (493398, 117)\n",
      "[PROGRESS] Starting load_min_model_helper()\n",
      "[DEBUG] MPATH /projects/leelab2/hughchen/RELIC/repr_learning/models/multivariate_mlp_labeldesat_bool92_5_nodesat_dtypeema[top15]+nonsignal_hd0_50ep_1000ba_adamopt_binary_crossentropyloss/\n",
      "[DEBUG] Loading model from /projects/leelab2/hughchen/RELIC/repr_learning//results/mlp_desat_bool92_5_nodesat_top15/hosp0_data/ema[top15]+nonsignal/\n",
      "[DEBUG] auc_lst.mean(): 0.0439282283491\n",
      "[DEBUG] roc_auc_lst.mean(): 0.75025382727\n",
      "\n",
      "[Progress] data_type raw[top15]+nonsignal\n",
      "[DEBUG] Y.shape: (3920564,)\n",
      "[DEBUG] Starting load_raw_data\n",
      "[Progress] trainvalX.shape (3920564, 949)\n",
      "[PROGRESS] Starting create_model()\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 23s 6us/step - loss: 0.3885 - val_loss: 0.1680\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 23s 6us/step - loss: 0.1761 - val_loss: 0.1679\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 23s 7us/step - loss: 0.1759 - val_loss: 0.1674\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 23s 6us/step - loss: 0.1739 - val_loss: 0.1644\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.1699 - val_loss: 0.1582\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.1638 - val_loss: 0.1527\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.1404 - val_loss: 0.1131\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.1088 - val_loss: 0.1014\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0987 - val_loss: 0.0933\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0912 - val_loss: 0.0874\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0849 - val_loss: 0.0819\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0794 - val_loss: 0.0771\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0749 - val_loss: 0.0739\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0714 - val_loss: 0.0701\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0686 - val_loss: 0.0684\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0663 - val_loss: 0.0659\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "1177000/3528507 [=========>....................] - ETA: 13s - loss: 0.0644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0574 - val_loss: 0.0570\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0569 - val_loss: 0.0566\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0565 - val_loss: 0.0562\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0562 - val_loss: 0.0559\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0559 - val_loss: 0.0559\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0556 - val_loss: 0.0558\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0554 - val_loss: 0.0551\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0551 - val_loss: 0.0550\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0549 - val_loss: 0.0547\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0547 - val_loss: 0.0547\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0545 - val_loss: 0.0562\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0543 - val_loss: 0.0540\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0541 - val_loss: 0.0559\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0539 - val_loss: 0.0538\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0538 - val_loss: 0.0577\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0537 - val_loss: 0.0538\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0535 - val_loss: 0.0539\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0534 - val_loss: 0.0547\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0533 - val_loss: 0.0534\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0531 - val_loss: 0.0551\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0531 - val_loss: 0.0540\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0529 - val_loss: 0.0535\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0529 - val_loss: 0.0527\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0528 - val_loss: 0.0537\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 21s 6us/step - loss: 0.0526 - val_loss: 0.0527\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 22s 6us/step - loss: 0.0526 - val_loss: 0.0530\n",
      "[DEBUG] Y.shape: (493398,)\n",
      "[DEBUG] Starting load_raw_data\n",
      "[Progress] test1X.shape    (493398, 949)\n",
      "[PROGRESS] Starting load_min_model_helper()\n",
      "[DEBUG] MPATH /projects/leelab2/hughchen/RELIC/repr_learning/models/multivariate_mlp_labeldesat_bool92_5_nodesat_dtyperaw[top15]+nonsignal_hd0_50ep_1000ba_adamopt_binary_crossentropyloss/\n",
      "[DEBUG] Loading model from /projects/leelab2/hughchen/RELIC/repr_learning//results/mlp_desat_bool92_5_nodesat_top15/hosp0_data/raw[top15]+nonsignal/\n",
      "[DEBUG] auc_lst.mean(): 0.04823439339\n",
      "[DEBUG] roc_auc_lst.mean(): 0.750943708705\n",
      "\n",
      "[Progress] data_type randemb[top15]+nonsignal\n",
      "[DEBUG] Y.shape: (3920564,)\n",
      "[DEBUG] HIDPATH /homes/gws/hughchen/phase/downstream_prediction//data/desat_bool92_5_nodesat/hospital_0/hidden200_200epochs/randemb/model_0/\n",
      "[Progress] trainvalX.shape (3920564, 3049)\n",
      "[PROGRESS] Starting create_model()\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 55s 16us/step - loss: 0.3001 - val_loss: 0.0574\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 57s 16us/step - loss: 0.0549 - val_loss: 0.0502\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 64s 18us/step - loss: 0.0490 - val_loss: 0.0461\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 52s 15us/step - loss: 0.0463 - val_loss: 0.0447\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0456 - val_loss: 0.0445\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0454 - val_loss: 0.0441\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0452 - val_loss: 0.0440\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0450 - val_loss: 0.0439\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0449 - val_loss: 0.0438\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0448 - val_loss: 0.0438\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 54s 15us/step - loss: 0.0447 - val_loss: 0.0437\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 54s 15us/step - loss: 0.0447 - val_loss: 0.0436\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0446 - val_loss: 0.0435\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0445 - val_loss: 0.0435\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 54s 15us/step - loss: 0.0444 - val_loss: 0.0435\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0444 - val_loss: 0.0435\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 54s 15us/step - loss: 0.0443 - val_loss: 0.0434\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0443 - val_loss: 0.0435\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0442 - val_loss: 0.0434\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0442 - val_loss: 0.0433\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0441 - val_loss: 0.0433\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0441 - val_loss: 0.0433\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0440 - val_loss: 0.0433\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 52s 15us/step - loss: 0.0440 - val_loss: 0.0432\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0439 - val_loss: 0.0432\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0439 - val_loss: 0.0431\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0439 - val_loss: 0.0431\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0438 - val_loss: 0.0430\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "3528507/3528507 [==============================] - 53s 15us/step - loss: 0.0438 - val_loss: 0.0431\n",
      "Train on 3528507 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "1077000/3528507 [========>.....................] - ETA: 34s - loss: 0.0438"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from xgb_setup import *\n",
    "os.nice(5)\n",
    "PATH = \"/projects/leelab2/hughchen/RELIC/repr_learning/\"\n",
    "DPATH = \"/homes/gws/hughchen/phase/downstream_prediction/\"\n",
    "RESULTPATH = PATH+\"/results/\"; MODELPATH = PATH+\"/models/\"\n",
    "lookback = 60\n",
    "DEBUG = False\n",
    "\n",
    "# label_type_eta_currfeat_lst = [(\"desat_bool92_5_nodesat\",0.02,\"SAO2\"),\n",
    "#                                (\"nibpm60\",0.1,\"NIBPM\"), \n",
    "#                                (\"etco235\",0.1,\"ETCO2\")]\n",
    "\n",
    "label_type_eta_currfeat_lst = [(\"nibpm60\",0.1,\"NIBPM\")]\n",
    "\n",
    "for label_type, _, curr_feat in label_type_eta_currfeat_lst:\n",
    "    print(\"\\n[Progress] label_type: {}, eta: {}, curr_feat {}\".format(label_type, \"NA\", curr_feat))\n",
    "\n",
    "    xgb_type = \"mlp_{}_top15\".format(label_type)\n",
    "    RESDIR = '{}{}/'.format(RESULTPATH, xgb_type)\n",
    "    if not os.path.exists(RESDIR): os.makedirs(RESDIR)\n",
    "\n",
    "    for hosp_data in [0,1]:\n",
    "        print(\"\\n[Progress] hosp_data {}\".format(hosp_data))\n",
    "\n",
    "        data_type = \"raw[top15]+nonsignal\"\n",
    "\n",
    "        print(\"\\n[Progress] data_type {}\".format(data_type))\n",
    "        (trainvalX,trainvalY) = load_data(DPATH,data_type,label_type,True,\n",
    "                                          hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "        print(\"[Progress] trainvalX.shape {}\".format(trainvalX.shape))\n",
    "        if not DEBUG:\n",
    "            MODDIR = train_mlp_model(RESDIR,trainvalX,trainvalY,\n",
    "                                     data_type,label_type,hosp_data)\n",
    "\n",
    "        (test1X,test1Y)       = load_data(DPATH,data_type,label_type,False,\n",
    "                                          hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "        print(\"[Progress] test1X.shape    {}\".format(test1X.shape))\n",
    "        if not DEBUG:\n",
    "            load_mlp_model_and_test(RESDIR,MODDIR,test1X,test1Y,\n",
    "                                    data_type,label_type,hosp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
