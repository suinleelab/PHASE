{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0929 17:26:25.804311 139970483787584 deprecation_wrapper.py:119] From ../embedding_utils.py:16: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0929 17:26:25.806766 139970483787584 deprecation_wrapper.py:119] From ../embedding_utils.py:16: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "W0929 17:26:25.807962 139970483787584 deprecation_wrapper.py:119] From ../embedding_utils.py:17: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:2_featname:ECGRATE.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:2_featname:ECGRATE.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_ECGRATEnextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_ECGRATEnextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:3_featname:ETCO2.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:3_featname:ETCO2.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_ETCO2nextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_ETCO2nextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:10_featname:ETSEV.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:10_featname:ETSEV.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_ETSEVnextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_ETSEVnextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:9_featname:ETSEVO.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:9_featname:ETSEVO.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_ETSEVOnextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_ETSEVOnextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:1_featname:FIO2.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:1_featname:FIO2.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_FIO2nextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_FIO2nextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:29_featname:NIBPD.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:29_featname:NIBPD.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_NIBPDnextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_NIBPDnextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:27_featname:NIBPM.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:27_featname:NIBPM.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_NIBPMnextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_NIBPMnextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:28_featname:NIBPS.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:28_featname:NIBPS.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_NIBPSnextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_NIBPSnextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:7_featname:PEAK.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:7_featname:PEAK.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_PEAKnextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_PEAKnextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:5_featname:PEEP.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:5_featname:PEEP.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_PEEPnextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_PEEPnextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:8_featname:PIP.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:8_featname:PIP.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_PIPnextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_PIPnextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:4_featname:RESPRATE.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:4_featname:RESPRATE.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_RESPRATEnextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_RESPRATEnextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_sao2minimum5_featnum:0_featname:SAO2.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_sao2minimum5_featnum:0_featname:SAO2.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_SAO2nextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_SAO2nextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:11_featname:TEMP1.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:11_featname:TEMP1.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_TEMP1nextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_TEMP1nextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n",
      "[PROGRESS] Starting load_train_val_data()\n",
      "[DEBUG] Loading from DATAPATH: /homes/gws/hughchen/phase/upstream_embedding/data/min5_data/SAO2minimum5/hospital_1/proc/\n",
      "[DEBUG] Loading train_x_file: X_train_60_SAO2minimum5_featnum:6_featname:TV.npy\n",
      "[DEBUG] Loading val_x_file  : X_val_60_SAO2minimum5_featnum:6_featname:TV.npy\n",
      "[DEBUG] Loading train_y_file: y_train_60_TVnextfive.npy\n",
      "[DEBUG] Loading val_y_file  : y_val_60_TVnextfive.npy\n",
      "[DEBUG] Saving processed y files\n",
      "[DEBUG] train_x.shape: (3785233, 60, 1), train_y.shape: (3785233, 5)\n",
      "[DEBUG] val_x.shape  : (420582, 60, 1),   val_y.shape: (420582, 5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4\"\n",
    "\n",
    "from embedding_utils import *\n",
    "\n",
    "########## Data setup ##########\n",
    "PATH = \"/homes/gws/hughchen/phase/upstream_embedding/\"\n",
    "hosp_data = 1\n",
    "DATAPATH  = '{}data/min5_data/{}minimum5/hospital_{}/proc/'.format(PATH,\"SAO2\",hosp_data)\n",
    "task = \"nextfive\"\n",
    "\n",
    "########## Form Data #########\n",
    "X_train_lst = []; X_valid_lst = []\n",
    "y_train_lst = []; y_valid_lst = []\n",
    "\n",
    "for i in range(0,len(X_lst)):\n",
    "    feat = X_lst[i]\n",
    "    \n",
    "    data = load_train_val_data(DATAPATH,task,feat=feat,save_proc_y=True,\n",
    "                               filter_zeros=False,impute_per_sample=False)\n",
    "    X_train, y_train, X_valid, y_valid = data\n",
    "\n",
    "    X_train_lst.append(X_train)\n",
    "    X_valid_lst.append(X_valid)\n",
    "    y_train_lst.append(y_train)\n",
    "    y_valid_lst.append(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0929 17:29:44.774096 139970483787584 deprecation_wrapper.py:119] From /homes/gws/hughchen/anaconda2/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0929 17:29:44.775962 139970483787584 deprecation_wrapper.py:119] From /homes/gws/hughchen/anaconda2/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Starting create_train_model()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0929 17:29:45.484375 139970483787584 deprecation.py:506] From /homes/gws/hughchen/anaconda2/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0929 17:30:19.231067 139970483787584 deprecation_wrapper.py:119] From /homes/gws/hughchen/anaconda2/envs/tf36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Starting train_model()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0929 17:30:26.136471 139970483787584 deprecation.py:323] From /homes/gws/hughchen/anaconda2/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300000 samples, validate on 420582 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 2033s 7ms/step - loss: 11.8529 - dense_1_loss: 0.6943 - dense_2_loss: 0.7002 - dense_3_loss: 0.8320 - dense_4_loss: 0.8320 - dense_5_loss: 0.7042 - dense_6_loss: 0.9597 - dense_7_loss: 0.9579 - dense_8_loss: 0.9560 - dense_9_loss: 0.7797 - dense_10_loss: 0.7329 - dense_11_loss: 0.7796 - dense_12_loss: 0.6995 - dense_13_loss: 0.9819 - dense_14_loss: 0.7482 - dense_15_loss: 0.4950 - val_loss: 11.3902 - val_dense_1_loss: 0.7074 - val_dense_2_loss: 0.6846 - val_dense_3_loss: 0.7733 - val_dense_4_loss: 0.7734 - val_dense_5_loss: 0.6137 - val_dense_6_loss: 0.9718 - val_dense_7_loss: 0.9452 - val_dense_8_loss: 0.9622 - val_dense_9_loss: 0.7839 - val_dense_10_loss: 0.6639 - val_dense_11_loss: 0.7839 - val_dense_12_loss: 0.7020 - val_dense_13_loss: 0.8791 - val_dense_14_loss: 0.6986 - val_dense_15_loss: 0.4472\n",
      "Train on 300000 samples, validate on 420582 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 1917s 6ms/step - loss: 11.5454 - dense_1_loss: 0.6794 - dense_2_loss: 0.6770 - dense_3_loss: 0.8121 - dense_4_loss: 0.8121 - dense_5_loss: 0.6603 - dense_6_loss: 0.9464 - dense_7_loss: 0.9441 - dense_8_loss: 0.9340 - dense_9_loss: 0.7710 - dense_10_loss: 0.7074 - dense_11_loss: 0.7710 - dense_12_loss: 0.6808 - dense_13_loss: 0.9319 - dense_14_loss: 0.7453 - dense_15_loss: 0.4727 - val_loss: 11.3105 - val_dense_1_loss: 0.7047 - val_dense_2_loss: 0.6782 - val_dense_3_loss: 0.7693 - val_dense_4_loss: 0.7693 - val_dense_5_loss: 0.6104 - val_dense_6_loss: 0.9625 - val_dense_7_loss: 0.9321 - val_dense_8_loss: 0.9546 - val_dense_9_loss: 0.7815 - val_dense_10_loss: 0.6630 - val_dense_11_loss: 0.7814 - val_dense_12_loss: 0.6991 - val_dense_13_loss: 0.8647 - val_dense_14_loss: 0.6950 - val_dense_15_loss: 0.4450\n",
      "Train on 300000 samples, validate on 420582 samples\n",
      "Epoch 1/1\n",
      "280000/300000 [===========================>..] - ETA: 1:23 - loss: 11.4953 - dense_1_loss: 0.6690 - dense_2_loss: 0.6734 - dense_3_loss: 0.8085 - dense_4_loss: 0.8085 - dense_5_loss: 0.6548 - dense_6_loss: 0.9284 - dense_7_loss: 0.9258 - dense_8_loss: 0.9208 - dense_9_loss: 0.7738 - dense_10_loss: 0.7071 - dense_11_loss: 0.7738 - dense_12_loss: 0.6762 - dense_13_loss: 0.9544 - dense_14_loss: 0.7443 - dense_15_loss: 0.4766"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from matplotlib import cm, pyplot as plt\n",
    "from sklearn import metrics\n",
    "from os.path import expanduser as eu\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import random, time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto(allow_soft_placement=True,gpu_options = tf.GPUOptions(allow_growth=True))\n",
    "set_session(tf.Session(config=config))\n",
    "GPUNUM = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\"))\n",
    "\n",
    "ISTUNE = False\n",
    "opt_name=\"rmsprop\";lr=0.001;drop=0.5\n",
    "b_size=1000;epoch_num=200;nodesize=200\n",
    "per_epoch_size = 300000\n",
    "\n",
    "# Fixed hyperpara\n",
    "print(\"[PROGRESS] Starting create_train_model()\")\n",
    "h1 = nodesize; h2 = nodesize\n",
    "lookback = 60; loss_func = \"mse\"\n",
    "\n",
    "# Set opt based on opt_name\n",
    "if opt_name is \"rmsprop\":\n",
    "    opt = keras.optimizers.RMSprop(lr)\n",
    "elif opt_name is \"sgd\":\n",
    "    opt = keras.optimizers.SGD(lr)\n",
    "elif opt_name is \"adam\":\n",
    "    opt = keras.optimizers.Adam(lr)\n",
    "\n",
    "# Form the model name (for saving the model)\n",
    "mod_name  = \"multivariate_biglstmdropout_hd{}_{}task_{}n_{}n_{}ep\".format(hosp_data,task,h1,h2,epoch_num)\n",
    "mod_name += \"_{}opt_{}lr\".format(opt_name,lr)\n",
    "mod_name += \"_{}drop_{}bs_epochsize\".format(drop,b_size,per_epoch_size)\n",
    "if ISTUNE:\n",
    "    MODDIR = PATH+\"models/tune_biglstm/\"+mod_name+\"/\"\n",
    "else:\n",
    "    MODDIR = PATH+\"models/\"+mod_name+\"/\"\n",
    "    \n",
    "loss_keys = [\"loss\", 'val_loss']\n",
    "[loss_keys.append(\"val_dense_{}_loss\".format(i)) for i in range(1,16)]\n",
    "\n",
    "if not os.path.exists(MODDIR): \n",
    "    os.makedirs(MODDIR)\n",
    "    with open(MODDIR+\"loss.txt\", \"w\") as f:\n",
    "        f.write(\"\\t\".join([\"i\", \"epoch_time\"] + loss_keys)+\"\\n\")\n",
    "\n",
    "    ########## Form Model #########\n",
    "    sig_lst     = []; encoded_lst = []\n",
    "    for i in range(0,len(X_lst)):\n",
    "        sig = Input(shape=(lookback,1))\n",
    "        lstm1 = LSTM(h1, recurrent_dropout=drop, return_sequences=True)\n",
    "        lstm2 = LSTM(h2, recurrent_dropout=drop, dropout=drop)\n",
    "\n",
    "        encoded = lstm2(lstm1(sig))\n",
    "        sig_lst.append(sig); encoded_lst.append(encoded)\n",
    "    merged_vector = keras.layers.concatenate(encoded_lst, axis=-1)\n",
    "\n",
    "    prediction_lst = []\n",
    "    for i in range(0,len(X_lst)):\n",
    "        prediction = Dense(5, activation='sigmoid')(merged_vector)\n",
    "        prediction_lst.append(prediction)\n",
    "\n",
    "    model = Model(inputs=sig_lst, outputs=prediction_lst)\n",
    "    model = multi_gpu_model(model,gpus=GPUNUM)\n",
    "    model.compile(optimizer=opt, loss=loss_func)\n",
    "    \n",
    "    ########## Training #########\n",
    "    print(\"[PROGRESS] Starting train_model()\")\n",
    "\n",
    "    # Train and Save\n",
    "    start_time = time.time()\n",
    "    for i in range(0,epoch_num):\n",
    "        train_subset_inds = np.random.choice(X_train_lst[0].shape[0],per_epoch_size,replace=False)\n",
    "        X_train_lst_sub = [X[train_subset_inds] for X in X_train_lst]\n",
    "        y_train_lst_sub = [y[train_subset_inds] for y in y_train_lst]\n",
    "        history = model.fit(X_train_lst_sub, y_train_lst_sub, epochs=1, batch_size=b_size, \n",
    "                            validation_data=(X_valid_lst,y_valid_lst))\n",
    "\n",
    "        # Save details about training\n",
    "        epoch_time = time.time() - start_time\n",
    "        write_lst = [i, epoch_time]\n",
    "        write_lst += [history.history[k][0] for k in loss_keys]\n",
    "        with open(MODDIR+\"loss.txt\", \"a\") as f:\n",
    "            f.write(\"\\t\".join([str(round(e,5)) for e in write_lst])+\"\\n\")\n",
    "\n",
    "        # Save model each iteration\n",
    "        val_loss = history.history['val_loss'][0]\n",
    "        model.save(\"{}val_loss:{}_epoch:{}_{}.h5\".format(MODDIR,val_loss,i,mod_name))\n",
    "\n",
    "# # Return the best validation performance (if hyperpara tuning)\n",
    "# min_mod_name = [f for f in sorted(os.listdir(MODDIR)) if \"val_loss\" in f][0]\n",
    "# best_val = float(min_mod_name.split(\"val_loss:\")[1].split(\"_\")[0])\n",
    "# return(best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for i in range(50,200):\n",
    "    train_subset_inds = np.random.choice(X_train_lst[0].shape[0],per_epoch_size,replace=False)\n",
    "    X_train_lst_sub = [X[train_subset_inds] for X in X_train_lst]\n",
    "    y_train_lst_sub = [y[train_subset_inds] for y in y_train_lst]\n",
    "    history = model.fit(X_train_lst_sub, y_train_lst_sub, epochs=1, batch_size=b_size, \n",
    "                        validation_data=(X_valid_lst,y_valid_lst))\n",
    "\n",
    "    # Save details about training\n",
    "    epoch_time = time.time() - start_time\n",
    "    write_lst = [i, epoch_time]\n",
    "    write_lst += [history.history[k][0] for k in loss_keys]\n",
    "    with open(MODDIR+\"loss.txt\", \"a\") as f:\n",
    "        f.write(\"\\t\".join([str(round(e,5)) for e in write_lst])+\"\\n\")\n",
    "\n",
    "    # Save model each iteration\n",
    "    val_loss = history.history['val_loss'][0]\n",
    "    model.save(\"{}val_loss:{}_epoch:{}_{}.h5\".format(MODDIR,val_loss,i,mod_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the difference in loss due to the imputation?\n",
    "\n",
    "### Compare the y labels to the upstream embedding ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,4\"\n",
    "\n",
    "import numpy as np\n",
    "from tbi_downstream_prediction import split_data\n",
    "\n",
    "PATH = \"/homes/gws/hughchen/phase/tbi_subset/\"\n",
    "DPATH = PATH+\"tbi/processed_data/hypoxemia/\"\n",
    "data_type = \"raw[top11]\"\n",
    "\n",
    "feat_lst = [\"ECGRATE\", \"ETCO2\", \"ETSEV\", \"ETSEVO\", \"FIO2\", \"NIBPD\", \"NIBPM\", \n",
    "            \"NIBPS\",\"PEAK\", \"PEEP\", \"PIP\", \"RESPRATE\", \"SAO2\", \"TEMP1\", \"TV\"]\n",
    "\n",
    "# Exclude these features\n",
    "weird_feat_lst = [\"ETSEV\", \"PIP\", \"PEEP\", \"TV\"]\n",
    "feat_inds = np.array([feat_lst.index(feat) for feat in feat_lst if feat not in weird_feat_lst])\n",
    "feat_lst2 = [feat for feat in feat_lst if feat not in weird_feat_lst]\n",
    "\n",
    "y_tbi = np.load(DPATH+\"tbiy.npy\")\n",
    "X_tbi = np.load(DPATH+\"X_tbi_imp_standard.npy\")\n",
    "\n",
    "X_tbi2 = X_tbi[:,feat_inds,:]\n",
    "(X_test, y_test, X_valid, y_valid, X_train, y_train) = split_data(DPATH,X_tbi2,y_tbi,flatten=False)\n",
    "\n",
    "PATH = \"/homes/gws/hughchen/phase/tbi_subset/\"\n",
    "RESULTPATH = PATH+\"results/\"\n",
    "label_type = \"desat_bool92_5_nodesat\"\n",
    "lstm_type = \"biglstmdropoutv3_{}\".format(label_type)\n",
    "RESDIR = '{}{}/'.format(RESULTPATH, lstm_type)\n",
    "if not os.path.exists(RESDIR): os.makedirs(RESDIR)\n",
    "\n",
    "import keras\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from matplotlib import cm, pyplot as plt\n",
    "from sklearn import metrics\n",
    "from os.path import expanduser as eu\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import random, time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto(allow_soft_placement=True,gpu_options = tf.GPUOptions(allow_growth=True))\n",
    "set_session(tf.Session(config=config))\n",
    "GPUNUM = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\"))\n",
    "\n",
    "def load_min_model_helper(MPATH):\n",
    "    print(\"[PROGRESS] Starting load_min_model_helper()\")\n",
    "    print(\"[DEBUG] MPATH {}\".format(MPATH))\n",
    "    mfiles = os.listdir(MPATH)\n",
    "    full_mod_name = MPATH.split(\"/\")[-1]\n",
    "    mfiles = [f for f in mfiles if \"val_loss\" in f]\n",
    "    loss_lst = [float(f.split(\"val_loss:\")[1].split(\"_\")[0]) for f in mfiles]\n",
    "    min_ind = loss_lst.index(min(loss_lst))\n",
    "    min_mod_name = \"{}/{}\".format(MPATH,mfiles[min_ind])\n",
    "    print(\"[DEBUG] min_mod_name {}\".format(mfiles[min_ind]))\n",
    "    return(load_model(min_mod_name))\n",
    "\n",
    "def create_train_model(opt_name=\"rmsprop\",lr=0.001,drop=0.5,\n",
    "                       b_size=1000,epoch_num=50,is_tune=False,\n",
    "                       nodesize=200):\n",
    "    # Fixed hyperpara\n",
    "    print(\"[PROGRESS] Starting create_train_model()\")\n",
    "    h1 = nodesize\n",
    "    h2 = nodesize\n",
    "    lookback = 60\n",
    "    loss_func = \"binary_crossentropy\"\n",
    "\n",
    "    # Set opt based on opt_name\n",
    "    if opt_name is \"rmsprop\":\n",
    "        opt = keras.optimizers.RMSprop(lr)\n",
    "    elif opt_name is \"sgd\":\n",
    "        opt = keras.optimizers.SGD(lr)\n",
    "    elif opt_name is \"adam\":\n",
    "        opt = keras.optimizers.Adam(lr)\n",
    "\n",
    "    # Form the model name (for saving the model)\n",
    "    mod_name  = \"multivariate_biglstmdropoutv3_{}n_{}n_{}ep\".format(h1,h2,epoch_num)\n",
    "    mod_name += \"_{}opt_{}lr\".format(opt_name,lr)\n",
    "    mod_name += \"_{}drop_{}bs\".format(drop,b_size)\n",
    "    if ISTUNE:\n",
    "        MODDIR = PATH+\"models/tune_biglstm/\"+mod_name+\"/\"\n",
    "    else:\n",
    "        MODDIR = PATH+\"models/\"+mod_name+\"/\"\n",
    "    if is_tune: MODDIR = PATH+\"models/tune_biglstm/\"+mod_name+\"/\"\n",
    "    if not os.path.exists(MODDIR): \n",
    "        os.makedirs(MODDIR)\n",
    "        with open(MODDIR+\"loss.txt\", \"w\") as f:\n",
    "            f.write(\"%s\\t%s\\t%s\\t%s\\n\" % (\"i\", \"train_loss\", \"val_loss\", \"epoch_time\"))\n",
    "\n",
    "        ########## Form Model/Data #########\n",
    "        X_train_lst = []; X_valid_lst = []; X_test_lst  = []\n",
    "        sig_lst     = []; encoded_lst = []\n",
    "\n",
    "        for i in range(0,len(feat_lst2)):\n",
    "            X_train_lst.append(X_train[:,:,i:(i+1)])\n",
    "            X_valid_lst.append(X_valid[:,:,i:(i+1)])\n",
    "            X_test_lst.append(X_test[:,:,i:(i+1)])\n",
    "\n",
    "            sig = Input(shape=(lookback,1))\n",
    "            lstm1 = LSTM(h1, recurrent_dropout=drop, return_sequences=True)\n",
    "            lstm2 = LSTM(h2, recurrent_dropout=drop, dropout=drop)\n",
    "\n",
    "            encoded = lstm2(lstm1(sig))\n",
    "            sig_lst.append(sig); encoded_lst.append(encoded)\n",
    "\n",
    "        merged_vector = keras.layers.concatenate(encoded_lst, axis=-1)\n",
    "        predictions = Dense(1, activation='sigmoid')(merged_vector)\n",
    "\n",
    "        model = Model(inputs=sig_lst, outputs=predictions)\n",
    "        model = multi_gpu_model(model, gpus=GPUNUM)\n",
    "        model.compile(optimizer=opt, loss=loss_func)\n",
    "\n",
    "        ########## Training #########\n",
    "        print(\"[PROGRESS] Starting train_model()\")\n",
    "\n",
    "        # Train and Save\n",
    "        start_time = time.time()\n",
    "        for i in range(0,epoch_num):\n",
    "            history = model.fit(X_train_lst, y_train, epochs=1, batch_size=b_size, \n",
    "                                validation_data=(X_valid_lst,y_valid))\n",
    "\n",
    "            # Save details about training\n",
    "            train_loss = history.history['loss'][0]\n",
    "            val_loss = history.history['val_loss'][0]\n",
    "            epoch_time = time.time() - start_time\n",
    "            with open(MODDIR+\"loss.txt\", \"a\") as f:\n",
    "                f.write(\"%d\\t%f\\t%f\\t%f\\n\" % (i, train_loss, val_loss, epoch_time))\n",
    "\n",
    "            # Save model each iteration\n",
    "            model.save(\"{}val_loss:{}_epoch:{}_{}.h5\".format(MODDIR,val_loss,i,mod_name))\n",
    "        \n",
    "    # Return the best validation performance (if hyperpara tuning)\n",
    "    min_mod_name = [f for f in sorted(os.listdir(MODDIR)) if \"val_loss\" in f][0]\n",
    "    best_val = float(min_mod_name.split(\"val_loss:\")[1].split(\"_\")[0])\n",
    "    return(best_val)\n",
    "\n",
    "def eval_test_model(opt_name=\"rmsprop\",lr=0.001,drop=0.5,\n",
    "                    b_size=1000,epoch_num=50,nodesize=200):    \n",
    "    \n",
    "    # Fixed hyperpara\n",
    "    print(\"[PROGRESS] Starting create_train_model()\")\n",
    "    lookback = 60; h1 = 200; h2 = 200\n",
    "    loss_func = \"binary_crossentropy\"\n",
    "\n",
    "    # Form the model name (for saving the model)\n",
    "    mod_name  = \"multivariate_biglstmdropoutv3_{}n_{}n_{}ep\".format(h1,h2,epoch_num)\n",
    "    mod_name += \"_{}opt_{}lr\".format(opt_name,lr)\n",
    "    mod_name += \"_{}drop_{}bs\".format(drop,b_size)\n",
    "    MODDIR = PATH+\"models/\"+mod_name+\"/\"\n",
    "    \n",
    "    ########## Form Data #########\n",
    "    X_test_lst  = []\n",
    "\n",
    "    for i in range(0,len(feat_lst2)):\n",
    "        X_test_lst.append(X_test[:,:,i:(i+1)])\n",
    "    \n",
    "    ########## Testing (optional) #########\n",
    "    model = load_min_model_helper(MODDIR)\n",
    "    model = multi_gpu_model(model,gpus=GPUNUM)\n",
    "    hosp_data = \"tbi\"\n",
    "    save_path = RESDIR+\"hosp{}_data/{}/\".format(hosp_data,data_type)\n",
    "    if not os.path.exists(save_path): os.makedirs(save_path)\n",
    "    ypred = model.predict(X_test_lst)\n",
    "    np.save(save_path+\"ypred.npy\",ypred)\n",
    "    np.save(save_path+\"y_test.npy\",y_test)\n",
    "    auc = metrics.average_precision_score(y_test, ypred)\n",
    "    np.random.seed(231)\n",
    "    auc_lst = []\n",
    "    roc_auc_lst = []\n",
    "    \n",
    "    # Randomly sample the test set\n",
    "    for i in range(0,100):\n",
    "        inds = np.random.choice(X_test.shape[0], X_test.shape[0], replace=True)\n",
    "        auc = metrics.average_precision_score(y_test[inds], ypred[inds])\n",
    "        auc_lst.append(auc)\n",
    "        roc_auc = metrics.roc_auc_score(y_test[inds], ypred[inds])\n",
    "        roc_auc_lst.append(roc_auc)\n",
    "    auc_lst = np.array(auc_lst)\n",
    "    roc_auc_lst = np.array(roc_auc_lst)\n",
    "    print(\"[DEBUG] auc_lst.mean(): {}\".format(auc_lst.mean()))\n",
    "    print(\"[DEBUG] roc_auc_lst.mean(): {}\".format(roc_auc_lst.mean()))\n",
    "\n",
    "    SP = RESDIR+\"hosp{}_data/\".format(hosp_data)\n",
    "    f = open('{}conf_int_hospdata{}_prauc.txt'.format(SP,hosp_data),'a')\n",
    "    f.write(\"{}, {}+-{}\\n\".format(data_type,auc_lst.mean().round(4),2*np.std(auc_lst).round(4)))\n",
    "    f.close()\n",
    "    f = open('{}conf_int_hospdata{}_rocauc.txt'.format(SP,hosp_data),'a')\n",
    "    f.write(\"{}, {}+-{}\\n\".format(data_type,roc_auc_lst.mean().round(4),2*np.std(roc_auc_lst).round(4)))\n",
    "    f.close()\n",
    "    np.save(\"{}auc_lst\".format(save_path,data_type), auc_lst)\n",
    "    np.save(\"{}roc_auc_lst\".format(save_path,data_type), roc_auc_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set of hyperparameters to tune over\n",
    "# node_lst   = [50, 100, 200]\n",
    "# opt_lst    = [\"rmsprop\", \"adam\", \"sgd\"]\n",
    "# lr_lst     = [0.01, 0.001, 0.0001]\n",
    "# drop_lst   = [0.3, 0.5, 0.7]\n",
    "# ISTUNE = True\n",
    "\n",
    "# # Greedily find best opt\n",
    "# min_loss_lst = [\n",
    "#     create_train_model(opt_name=\"rmsprop\",lr=0.001,drop=0.5,\n",
    "#                        b_size=1000,nodesize=para)\n",
    "#     for para in node_lst\n",
    "# ]\n",
    "# best_nodesize = node_lst[min_loss_lst.index(min(min_loss_lst))]\n",
    "# print(\"best best_nodesize: {}\".format(best_nodesize))\n",
    "\n",
    "# # Greedily find best opt\n",
    "# min_loss_lst = [\n",
    "#     create_train_model(opt_name=para,lr=0.001,drop=0.5,\n",
    "#                        b_size=1000,nodesize=best_nodesize)\n",
    "#     for para in opt_lst\n",
    "# ]\n",
    "# best_optname = opt_lst[min_loss_lst.index(min(min_loss_lst))]\n",
    "# print(\"best optname: {}\".format(best_optname))\n",
    "\n",
    "# # Greedily find best lr\n",
    "# min_loss_lst = [\n",
    "#     create_train_model(opt_name=best_optname,lr=para,drop=0.5,\n",
    "#                        b_size=1000,nodesize=best_nodesize)\n",
    "#     for para in lr_lst\n",
    "# ]\n",
    "# best_lr = lr_lst[min_loss_lst.index(min(min_loss_lst))]\n",
    "# print(\"best lr: {}\".format(best_lr))\n",
    "\n",
    "# # Greedily find best dropout\n",
    "# min_loss_lst = [\n",
    "#     create_train_model(opt_name=best_optname,lr=best_lr,drop=para,\n",
    "#                        b_size=1000,nodesize=best_nodesize)\n",
    "#     for para in drop_lst\n",
    "# ]\n",
    "# best_drop = drop_lst[min_loss_lst.index(min(min_loss_lst))]\n",
    "# print(\"best drop: {}\".format(best_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_model(opt_name=best_optname,lr=best_lr,drop=best_drop,\n",
    "                   b_size=1000,nodesize=best_nodesize,epoch_num=200,\n",
    "                   is_tune=False)\n",
    "eval_test_model(opt_name=best_optname,lr=best_lr,drop=best_drop,\n",
    "                b_size=1000,nodesize=best_nodesize,epoch_num=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_test_model(opt_name=best_optname,lr=best_lr,drop=best_drop,\n",
    "                b_size=1000,nodesize=best_nodesize,epoch_num=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf36)",
   "language": "python",
   "name": "tf36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
