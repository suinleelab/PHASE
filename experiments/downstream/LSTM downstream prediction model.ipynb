{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM downstream prediction model\n",
    "\n",
    "Evaluate an LSTM downstream prediction model on the raw physiological signal embeddings and the static patient data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from matplotlib import cm, pyplot as plt\n",
    "from sklearn import metrics\n",
    "from os.path import expanduser as eu\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import keras\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto(allow_soft_placement=True,gpu_options = tf.GPUOptions(allow_growth=True))\n",
    "set_session(tf.Session(config=config))\n",
    "GPUNUM = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\"))\n",
    "\n",
    "def standardize_static(X_lst):\n",
    "    mean = X_lst[-1].mean(0)\n",
    "    std  = X_lst[-1].std(0)\n",
    "    std[std == 0] = 1\n",
    "    X_lst[-1] = (X_lst[-1] - mean)/std\n",
    "    return(X_lst)\n",
    "\n",
    "def load_min_model_helper(MPATH):\n",
    "    print(\"[PROGRESS] Starting load_min_model_helper()\")\n",
    "    print(\"[DEBUG] MPATH {}\".format(MPATH))\n",
    "    mfiles = os.listdir(MPATH)\n",
    "    full_mod_name = MPATH.split(\"/\")[-1]\n",
    "    mfiles = [f for f in mfiles if \"val_loss\" in f]\n",
    "    loss_lst = [float(f.split(\"val_loss:\")[1].split(\"_\")[0]) for f in mfiles]\n",
    "    min_ind = loss_lst.index(min(loss_lst))\n",
    "    min_mod_name = \"{}/{}\".format(MPATH,mfiles[min_ind])\n",
    "    if DEBUG: print(\"[DEBUG] min_mod_name {}\".format(mfiles[min_ind]))\n",
    "    return(load_model(min_mod_name))\n",
    "\n",
    "def split_data_lst(X_trval_lst,y_trval):\n",
    "    train_ratio = 0.9\n",
    "    sample_size = X_trval_lst[0].shape[0]\n",
    "    nine_tenths_ind = int(train_ratio*sample_size)\n",
    "    X_train = [X[0:nine_tenths_ind,:] for X in X_trval_lst]\n",
    "    y_train = y_trval[0:nine_tenths_ind]\n",
    "    X_valid = [X[nine_tenths_ind:sample_size,:] for X in X_trval_lst]\n",
    "    y_valid = y_trval[nine_tenths_ind:sample_size]\n",
    "    del X_trval_lst\n",
    "    gc.collect()\n",
    "    return(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "def form_train_model(X_trval_lst,y_trval,hyper_dict,\n",
    "                     label_type,epoch_num=50,is_tune=True):\n",
    "    ########## Form Data #########\n",
    "    X_train_lst, y_train, X_valid_lst, y_valid = split_data_lst(X_trval_lst,y_trval)\n",
    "\n",
    "    X_train_lst = [X[:,:,np.newaxis] if X.shape[1] == 60 else X for X in X_train_lst]\n",
    "    X_train_lst = [np.concatenate(X_train_lst[:-1],2),X_train_lst[-1]]\n",
    "\n",
    "    X_valid_lst = [X[:,:,np.newaxis] if X.shape[1] == 60 else X for X in X_valid_lst]\n",
    "    X_valid_lst = [np.concatenate(X_valid_lst[:-1],2),X_valid_lst[-1]]\n",
    "\n",
    "    ########## Form Model #########\n",
    "    print(\"[PROGRESS] form_model()\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    numlayer = hyper_dict[\"numlayer\"]\n",
    "    nodesize = hyper_dict[\"numnode\"]\n",
    "    opt_name = hyper_dict[\"opt\"]\n",
    "    drop     = hyper_dict[\"drop\"]\n",
    "    lr       = hyper_dict[\"lr\"]\n",
    "    \n",
    "    if opt_name == \"RMSprop\":\n",
    "        opt = RMSprop(lr);\n",
    "    elif opt_name == \"Adam\":\n",
    "        opt = Adam(lr);\n",
    "    elif opt_name == \"SGD\":\n",
    "        opt = SGD(lr);\n",
    "    \n",
    "    b_size = 1000\n",
    "    per_epoch_size = 300000\n",
    "\n",
    "    lookback = 60\n",
    "    loss_func = \"binary_crossentropy\"\n",
    "\n",
    "    # Model name\n",
    "    if is_tune:\n",
    "        mod_path  = \"tune_multilstm_{}hospdata_{}label\".format(hosp_data,label_type)\n",
    "        mod_name  = \"\".join([\"{}{}_\".format(hyper_dict[k],k) for k in hyper_dict])\n",
    "        mod_name += \"{}bsize_{}epochnum_{}perepochsize\".format(b_size,epoch_num,per_epoch_size)\n",
    "        MODDIR = \"{}models/{}/{}/\".format(PATH,mod_path,mod_name)\n",
    "    else:\n",
    "        mod_name  = \"multilstm_{}hospdata_{}label_\".format(hosp_data,label_type)\n",
    "        mod_name += \"\".join([\"{}{}_\".format(hyper_dict[k],k) for k in hyper_dict])\n",
    "        mod_name += \"{}bsize_{}epochnum_{}perepochsize\".format(b_size,epoch_num,per_epoch_size)\n",
    "        MODDIR = \"{}models/{}/\".format(PATH,mod_name)\n",
    "    \n",
    "    input_lst   = []; encoded_lst = []\n",
    "    # Signals\n",
    "    sig = Input(shape=(lookback,15))\n",
    "    if numlayer == 1:\n",
    "        lstm1 = LSTM(nodesize, recurrent_dropout=drop)\n",
    "        encoded = lstm1(sig)\n",
    "    elif numlayer == 2:\n",
    "        lstm1 = LSTM(nodesize, recurrent_dropout=drop, return_sequences=True)\n",
    "        lstm2 = LSTM(nodesize, recurrent_dropout=drop, dropout=drop)\n",
    "        encoded = lstm2(lstm1(sig))\n",
    "    elif numlayer == 3:\n",
    "        lstm1 = LSTM(nodesize, recurrent_dropout=drop, return_sequences=True)\n",
    "        lstm2 = LSTM(nodesize, recurrent_dropout=drop, dropout=drop, return_sequences=True)\n",
    "        lstm3 = LSTM(nodesize, recurrent_dropout=drop, dropout=drop)\n",
    "        encoded = lstm3(lstm2(lstm1(sig)))\n",
    "    else:\n",
    "        assert numlayer <= 3, \"Too many layers\"\n",
    "    input_lst.append(sig); encoded_lst.append(encoded)\n",
    "\n",
    "    # Static variables\n",
    "    static_size = X_trval_lst[-1].shape[1]\n",
    "    static = Input(shape=[static_size])\n",
    "    input_lst   += [static]; encoded_lst += [static]\n",
    "    \n",
    "    # Combine and compile model\n",
    "    merged_vector = keras.layers.concatenate(encoded_lst, axis=-1)\n",
    "    predictions = Dense(1, activation='sigmoid')(merged_vector)\n",
    "    model = Model(inputs=input_lst, outputs=predictions)\n",
    "    model.compile(optimizer=opt, loss=loss_func)\n",
    "    \n",
    "    ########## Training #########\n",
    "    print(\"[PROGRESS] Starting train_model()\")\n",
    "\n",
    "    loss_keys = [\"loss\", 'val_loss']\n",
    "\n",
    "    print(\"[PROGRESS] Making MODDIR {}\".format(MODDIR))\n",
    "    if not os.path.exists(MODDIR): os.makedirs(MODDIR)\n",
    "    with open(MODDIR+\"loss.txt\", \"w\") as f:\n",
    "        f.write(\"\\t\".join([\"i\", \"epoch_time\"] + loss_keys)+\"\\n\")\n",
    "\n",
    "    # Train and Save\n",
    "    start_time = time.time()\n",
    "    for i in range(0,epoch_num):\n",
    "        train_subset_inds = np.random.choice(X_train_lst[0].shape[0],per_epoch_size,replace=False)\n",
    "#         pos_inds = np.where(y_train==1)[0]\n",
    "#         neg_inds = np.where(y_train!=1)[0]\n",
    "#         neg_subset_inds = np.random.choice(neg_inds,pos_inds.shape[0],replace=False)\n",
    "#         train_subset_inds = np.concatenate([pos_inds,neg_subset_inds])\n",
    "        np.random.shuffle(train_subset_inds)\n",
    "        X_train_lst_sub = [X[train_subset_inds] for X in X_train_lst]\n",
    "        y_train_sub     = y_train[train_subset_inds]\n",
    "        history = model.fit(X_train_lst_sub, y_train_sub, epochs=1, batch_size=b_size, \n",
    "                            validation_data=(X_valid_lst,y_valid))\n",
    "\n",
    "        # Save details about training\n",
    "        epoch_time = time.time() - start_time\n",
    "        write_lst = [i, epoch_time]\n",
    "        write_lst += [history.history[k][0] for k in loss_keys]\n",
    "        with open(MODDIR+\"loss.txt\", \"a\") as f:\n",
    "            f.write(\"\\t\".join([str(round(e,5)) for e in write_lst])+\"\\n\")\n",
    "\n",
    "        # Save model each iteration\n",
    "        val_loss = history.history['val_loss'][0]\n",
    "        model.save(\"{}val_loss:{}_epoch:{}.h5\".format(MODDIR,val_loss,i))\n",
    "\n",
    "    return(MODDIR)\n",
    "\n",
    "def load_model_and_test(RESDIR,MODDIR,X_test,y_test,data_type,hosp_data):\n",
    "    model = load_min_model_helper(MODDIR)\n",
    "    save_path = RESDIR+\"hosp{}_data/{}/\".format(hosp_data,data_type)\n",
    "    if not os.path.exists(save_path): os.makedirs(save_path)\n",
    "    print(\"[DEBUG] Loading model from {}\".format(save_path))\n",
    "    ypred = model.predict(X_test)\n",
    "    np.save(save_path+\"ypred.npy\",ypred)\n",
    "    np.save(save_path+\"y_test.npy\",y_test)\n",
    "    auc = metrics.average_precision_score(y_test, ypred)\n",
    "    np.random.seed(231)\n",
    "    auc_lst = []\n",
    "    roc_auc_lst = []\n",
    "    for i in range(0,100):\n",
    "        inds = np.random.choice(X_test[-1].shape[0], X_test[-1].shape[0], replace=True)\n",
    "        auc = metrics.average_precision_score(y_test[inds], ypred[inds])\n",
    "        auc_lst.append(auc)\n",
    "        roc_auc = metrics.roc_auc_score(y_test[inds], ypred[inds])\n",
    "        roc_auc_lst.append(roc_auc)\n",
    "    auc_lst = np.array(auc_lst)\n",
    "    roc_auc_lst = np.array(roc_auc_lst)\n",
    "    print(\"[DEBUG] auc_lst.mean(): {}\".format(auc_lst.mean()))\n",
    "    print(\"[DEBUG] roc_auc_lst.mean(): {}\".format(roc_auc_lst.mean()))\n",
    "\n",
    "    SP = RESDIR+\"hosp{}_data/\".format(hosp_data)\n",
    "    f = open('{}conf_int_hospdata{}_prauc.txt'.format(SP,hosp_data),'a')\n",
    "    f.write(\"{}, {}+-{}\\n\".format(data_type,auc_lst.mean().round(4),2*np.std(auc_lst).round(4)))\n",
    "    f.close()\n",
    "    f = open('{}conf_int_hospdata{}_rocauc.txt'.format(SP,hosp_data),'a')\n",
    "    f.write(\"{}, {}+-{}\\n\".format(data_type,roc_auc_lst.mean().round(4),2*np.std(roc_auc_lst).round(4)))\n",
    "    f.close()\n",
    "    np.save(\"{}auc_lst\".format(save_path,data_type), auc_lst)\n",
    "    np.save(\"{}roc_auc_lst\".format(save_path,data_type), roc_auc_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from xgb_setup import *\n",
    "os.nice(5)\n",
    "PATH = \"/homes/gws/hughchen/phase/downstream_prediction/\"\n",
    "DPATH = PATH\n",
    "RESULTPATH = PATH+\"/results/\"\n",
    "MODELPATH  = PATH+\"/models/\"\n",
    "DEBUG = False\n",
    "\n",
    "# label_type_eta_currfeat_lst = [(\"desat_bool92_5_nodesat\",0.02,\"SAO2\"),\n",
    "#                                (\"nibpm60\",0.1,\"NIBPM\"), \n",
    "#                                (\"etco235\",0.1,\"ETCO2\")]\n",
    "\n",
    "label_type = \"desat_bool92_5_nodesat\"\n",
    "curr_feat = \"SAO2\"\n",
    "print(\"\\n[Progress] label_type: {}, curr_feat {}\".format(label_type, curr_feat))\n",
    "\n",
    "hosp_data = 0\n",
    "data_type = \"proc[top15]+nonsignal\"\n",
    "\n",
    "# Load train validation data and split it\n",
    "(X_trval_lst,y_trval) = load_data(DPATH,data_type,label_type,True,hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "X_trval_lst = standardize_static(X_trval_lst)\n",
    "\n",
    "numlayers = [1,2,3]\n",
    "numnodes  = [100,200,300]\n",
    "opt_lst   = [\"RMSprop\", \"SGD\", \"Adam\"]\n",
    "lr_lst    = [0.01,0.001,0.0001]\n",
    "drop_lst  = [0,0.5]\n",
    "\n",
    "for numlayer in numlayers:\n",
    "    for numnode in numnodes:\n",
    "        for opt in opt_lst[0:1]:\n",
    "            for lr in lr_lst:\n",
    "                for drop in drop_lst:\n",
    "                    hyper_dict = {\"numlayer\":numlayer,\"numnode\":numnode,\n",
    "                                  \"opt\":opt,\"lr\":lr,\"drop\":drop}\n",
    "                    form_train_model(X_trval_lst,y_trval,hyper_dict,label_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load min model and train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Progress] label_type: desat_bool92_5_nodesat, curr_feat SAO2\n",
      "[DEBUG] Y.shape: (3920564,)\n",
      "[DEBUG] Starting load_raw_data\n",
      "[DEBUG] DPATH /homes/gws/hughchen/phase/downstream_prediction//data/desat_bool92_5_nodesat/hospital_0/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1028 09:03:05.937915 140596101756736 deprecation_wrapper.py:119] From /homes/gws/hughchen/anaconda2/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1028 09:03:05.939880 140596101756736 deprecation_wrapper.py:119] From /homes/gws/hughchen/anaconda2/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1028 09:03:05.943795 140596101756736 deprecation_wrapper.py:119] From /homes/gws/hughchen/anaconda2/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] form_model()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1028 09:03:19.665680 140596101756736 deprecation_wrapper.py:119] From /homes/gws/hughchen/anaconda2/envs/tf36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1028 09:03:19.672633 140596101756736 deprecation_wrapper.py:119] From /homes/gws/hughchen/anaconda2/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1028 09:03:19.677962 140596101756736 deprecation.py:323] From /homes/gws/hughchen/anaconda2/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Starting train_model()\n",
      "[PROGRESS] Making MODDIR /homes/gws/hughchen/phase/downstream_prediction/models/multilstm_0hospdata_desat_bool92_5_nodesatlabel_3numlayer_300numnode_RMSpropopt_0.001lr_0.0drop_1000bsize_200epochnum_300000perepochsize/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1028 09:03:23.311719 140596101756736 deprecation_wrapper.py:119] From /homes/gws/hughchen/anaconda2/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300000 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 171s 570us/step - loss: 0.0550 - val_loss: 0.0437\n",
      "Train on 300000 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 166s 554us/step - loss: 0.0449 - val_loss: 0.0410\n",
      "Train on 300000 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 166s 552us/step - loss: 0.0407 - val_loss: 0.0396\n",
      "Train on 300000 samples, validate on 392057 samples\n",
      "Epoch 1/1\n",
      "299000/300000 [============================>.] - ETA: 0s - loss: 0.0412"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from xgb_setup import *\n",
    "os.nice(5)\n",
    "PATH = \"/homes/gws/hughchen/phase/downstream_prediction/\"\n",
    "DPATH = PATH\n",
    "RESULTPATH = PATH+\"/results/\"\n",
    "MODELPATH  = PATH+\"/models/\"\n",
    "DEBUG = False\n",
    "\n",
    "label_type = \"desat_bool92_5_nodesat\"\n",
    "curr_feat = \"SAO2\"\n",
    "print(\"\\n[Progress] label_type: {}, curr_feat {}\".format(label_type, curr_feat))\n",
    "\n",
    "hosp_data = 0\n",
    "data_type = \"proc[top15]+nonsignal\"\n",
    "\n",
    "# Get best model in terms of min validation loss\n",
    "TUNEPATH = MODELPATH+\"tune_multilstm_0hospdata_desat_bool92_5_nodesatlabel/\"\n",
    "mod_names = os.listdir(TUNEPATH)\n",
    "best_min_val_loss = float(\"inf\")\n",
    "for mod_name in mod_names:\n",
    "    CURRMODPATH = TUNEPATH+mod_name\n",
    "    model_checkpts = [f for f in os.listdir(CURRMODPATH) if \"val_loss\" in f]\n",
    "    min_val_loss = float(sorted(model_checkpts)[1].split(\"val_loss:\")[1].split(\"_\")[0])\n",
    "    if min_val_loss < best_min_val_loss:\n",
    "        best_min_val_loss = min_val_loss\n",
    "        best_mod_name = mod_name\n",
    "\n",
    "hyper_dict = {\"numlayer\" : int(best_mod_name.split(\"numlayer_\")[0]),\n",
    "              \"numnode\"  : int(best_mod_name.split(\"numnode_\")[0].split(\"_\")[-1]),\n",
    "              \"opt\"      : best_mod_name.split(\"opt_\")[0].split(\"_\")[-1],\n",
    "              \"lr\"       : float(best_mod_name.split(\"lr_\")[0].split(\"_\")[-1]),\n",
    "              \"drop\"     : float(best_mod_name.split(\"drop_\")[0].split(\"_\")[-1])}\n",
    "\n",
    "# Load train validation data and split it\n",
    "(X_trval_lst,y_trval) = load_data(DPATH,data_type,label_type,True,hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "X_trval_lst = standardize_static(X_trval_lst)\n",
    "\n",
    "# Train model\n",
    "MODDIR = form_train_model(X_trval_lst,y_trval,hyper_dict,label_type,is_tune=False,epoch_num=200)\n",
    "\n",
    "mod_type = \"multilstm_{}\".format(label_type)\n",
    "RESDIR = '{}{}/'.format(RESULTPATH, mod_type)\n",
    "if not os.path.exists(RESDIR): os.makedirs(RESDIR)\n",
    "\n",
    "# Load test data\n",
    "(X_test1_lst,y_test1) = load_data(DPATH,data_type,label_type,False,hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "X_test1_lst = standardize_static(X_test1_lst)\n",
    "X_test1_lst = [X[:,:,np.newaxis] if X.shape[1] == 60 else X for X in X_test1_lst]\n",
    "X_test1_lst = [np.concatenate(X_test1_lst[:-1],2),X_test1_lst[-1]]\n",
    "\n",
    "# Evaluate on the test set\n",
    "load_model_and_test(RESDIR,MODDIR,X_test1_lst,y_test,data_type,hosp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Y.shape: (493398,)\n",
      "[DEBUG] Starting load_raw_data\n",
      "[DEBUG] DPATH /homes/gws/hughchen/phase/downstream_prediction//data/desat_bool92_5_nodesat/hospital_0/\n",
      "[PROGRESS] Starting load_min_model_helper()\n",
      "[DEBUG] MPATH /homes/gws/hughchen/phase/downstream_prediction/models/multilstm_0hospdata_desat_bool92_5_nodesatlabel_3numlayer_300numnode_RMSpropopt_0.001lr_0.0drop_1000bsize_200epochnum_300000perepochsize/\n",
      "[DEBUG] Loading model from /homes/gws/hughchen/phase/downstream_prediction//results/multilstm_desat_bool92_5_nodesat/hosp0_data/proc[top15]+nonsignal/\n",
      "[DEBUG] auc_lst.mean(): 0.27563223488754607\n",
      "[DEBUG] roc_auc_lst.mean(): 0.911993681631015\n"
     ]
    }
   ],
   "source": [
    "epoch_num=200\n",
    "is_tune=False\n",
    "b_size = 1000\n",
    "per_epoch_size = 300000\n",
    "lookback = 60\n",
    "loss_func = \"binary_crossentropy\"\n",
    "\n",
    "# Model name\n",
    "if is_tune:\n",
    "    mod_path  = \"tune_multilstm_{}hospdata_{}label\".format(hosp_data,label_type)\n",
    "    mod_name  = \"\".join([\"{}{}_\".format(hyper_dict[k],k) for k in hyper_dict])\n",
    "    mod_name += \"{}bsize_{}epochnum_{}perepochsize\".format(b_size,epoch_num,per_epoch_size)\n",
    "    MODDIR = \"{}models/{}/{}/\".format(PATH,mod_path,mod_name)\n",
    "else:\n",
    "    mod_name  = \"multilstm_{}hospdata_{}label_\".format(hosp_data,label_type)\n",
    "    mod_name += \"\".join([\"{}{}_\".format(hyper_dict[k],k) for k in hyper_dict])\n",
    "    mod_name += \"{}bsize_{}epochnum_{}perepochsize\".format(b_size,epoch_num,per_epoch_size)\n",
    "    MODDIR = \"{}models/{}/\".format(PATH,mod_name)\n",
    "    \n",
    "mod_type = \"multilstm_{}\".format(label_type)\n",
    "RESDIR = '{}{}/'.format(RESULTPATH, mod_type)\n",
    "if not os.path.exists(RESDIR): os.makedirs(RESDIR)\n",
    "    \n",
    "# Load test data\n",
    "(X_test1_lst,y_test1) = load_data(DPATH,data_type,label_type,False,hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "X_test1_lst = standardize_static(X_test1_lst)\n",
    "X_test1_lst = [X[:,:,np.newaxis] if X.shape[1] == 60 else X for X in X_test1_lst]\n",
    "X_test1_lst = [np.concatenate(X_test1_lst[:-1],2),X_test1_lst[-1]]\n",
    "\n",
    "# Evaluate on the test set\n",
    "load_model_and_test(RESDIR,MODDIR,X_test1_lst,y_test1,data_type,hosp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Progress] label_type: desat_bool92_5_nodesat, curr_feat SAO2\n",
      "[DEBUG] Y.shape: (4167959,)\n",
      "[DEBUG] Starting load_raw_data\n",
      "[DEBUG] DPATH /homes/gws/hughchen/phase/downstream_prediction//data/desat_bool92_5_nodesat/hospital_1/\n",
      "[PROGRESS] form_model()\n",
      "[PROGRESS] Starting train_model()\n",
      "[PROGRESS] Making MODDIR /homes/gws/hughchen/phase/downstream_prediction/models/multilstm_1hospdata_desat_bool92_5_nodesatlabel_3numlayer_300numnode_RMSpropopt_0.001lr_0.0drop_1000bsize_200epochnum_300000perepochsize/\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 175s 583us/step - loss: 0.0987 - val_loss: 0.0934\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 171s 569us/step - loss: 0.0870 - val_loss: 0.0864\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 173s 576us/step - loss: 0.0838 - val_loss: 0.0861\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 173s 577us/step - loss: 0.0827 - val_loss: 0.0856\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 171s 570us/step - loss: 0.0827 - val_loss: 0.0850\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 171s 570us/step - loss: 0.0810 - val_loss: 0.0851\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 171s 571us/step - loss: 0.0824 - val_loss: 0.0846\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 170s 567us/step - loss: 0.0831 - val_loss: 0.0837\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 181s 604us/step - loss: 0.0803 - val_loss: 0.0845\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 187s 622us/step - loss: 0.0810 - val_loss: 0.0843\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 186s 620us/step - loss: 0.0802 - val_loss: 0.0836\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 184s 613us/step - loss: 0.0793 - val_loss: 0.0841\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 184s 612us/step - loss: 0.0797 - val_loss: 0.0834\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 178s 592us/step - loss: 0.0806 - val_loss: 0.0843\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 168s 559us/step - loss: 0.0794 - val_loss: 0.0843\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 171s 569us/step - loss: 0.0782 - val_loss: 0.0844\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 174s 578us/step - loss: 0.0789 - val_loss: 0.0843\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 173s 576us/step - loss: 0.0802 - val_loss: 0.0847\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 173s 577us/step - loss: 0.0805 - val_loss: 0.0838\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 174s 579us/step - loss: 0.0787 - val_loss: 0.0840\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 174s 581us/step - loss: 0.0802 - val_loss: 0.0843\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 174s 578us/step - loss: 0.0785 - val_loss: 0.0841\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 173s 575us/step - loss: 0.0787 - val_loss: 0.0842\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 174s 581us/step - loss: 0.0770 - val_loss: 0.0850\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 173s 576us/step - loss: 0.0776 - val_loss: 0.0842\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 171s 572us/step - loss: 0.0783 - val_loss: 0.0854\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 184s 612us/step - loss: 0.0772 - val_loss: 0.0854\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 182s 606us/step - loss: 0.0777 - val_loss: 0.0853\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 185s 616us/step - loss: 0.0770 - val_loss: 0.0858\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 185s 615us/step - loss: 0.0762 - val_loss: 0.0857\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 171s 570us/step - loss: 0.0760 - val_loss: 0.0863\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 187s 623us/step - loss: 0.0752 - val_loss: 0.0859\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 184s 613us/step - loss: 0.0754 - val_loss: 0.0860\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 174s 580us/step - loss: 0.0759 - val_loss: 0.0854\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 165s 550us/step - loss: 0.0764 - val_loss: 0.0866\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 165s 550us/step - loss: 0.0758 - val_loss: 0.0860\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 165s 551us/step - loss: 0.0736 - val_loss: 0.0873\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 164s 548us/step - loss: 0.0757 - val_loss: 0.0881\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 164s 548us/step - loss: 0.0736 - val_loss: 0.0875\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 164s 547us/step - loss: 0.0718 - val_loss: 0.0868\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 165s 551us/step - loss: 0.0722 - val_loss: 0.0890\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 164s 548us/step - loss: 0.0728 - val_loss: 0.0887\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 166s 554us/step - loss: 0.0729 - val_loss: 0.0898\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 165s 552us/step - loss: 0.0721 - val_loss: 0.0895\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "300000/300000 [==============================] - 165s 551us/step - loss: 0.0727 - val_loss: 0.0893\n",
      "Train on 300000 samples, validate on 416796 samples\n",
      "Epoch 1/1\n",
      "136000/300000 [============>.................] - ETA: 57s - loss: 0.0720"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from xgb_setup import *\n",
    "os.nice(5)\n",
    "PATH = \"/homes/gws/hughchen/phase/downstream_prediction/\"\n",
    "DPATH = PATH\n",
    "RESULTPATH = PATH+\"/results/\"\n",
    "MODELPATH  = PATH+\"/models/\"\n",
    "DEBUG = False\n",
    "\n",
    "label_type = \"desat_bool92_5_nodesat\"\n",
    "curr_feat = \"SAO2\"\n",
    "print(\"\\n[Progress] label_type: {}, curr_feat {}\".format(label_type, curr_feat))\n",
    "\n",
    "hosp_data = 1\n",
    "data_type = \"proc[top15]+nonsignal\"\n",
    "\n",
    "# Get best model in terms of min validation loss\n",
    "TUNEPATH = MODELPATH+\"tune_multilstm_0hospdata_desat_bool92_5_nodesatlabel/\"\n",
    "mod_names = os.listdir(TUNEPATH)\n",
    "best_min_val_loss = float(\"inf\")\n",
    "for mod_name in mod_names:\n",
    "    CURRMODPATH = TUNEPATH+mod_name\n",
    "    model_checkpts = [f for f in os.listdir(CURRMODPATH) if \"val_loss\" in f]\n",
    "    min_val_loss = float(sorted(model_checkpts)[1].split(\"val_loss:\")[1].split(\"_\")[0])\n",
    "    if min_val_loss < best_min_val_loss:\n",
    "        best_min_val_loss = min_val_loss\n",
    "        best_mod_name = mod_name\n",
    "\n",
    "hyper_dict = {\"numlayer\" : int(best_mod_name.split(\"numlayer_\")[0]),\n",
    "              \"numnode\"  : int(best_mod_name.split(\"numnode_\")[0].split(\"_\")[-1]),\n",
    "              \"opt\"      : best_mod_name.split(\"opt_\")[0].split(\"_\")[-1],\n",
    "              \"lr\"       : float(best_mod_name.split(\"lr_\")[0].split(\"_\")[-1]),\n",
    "              \"drop\"     : float(best_mod_name.split(\"drop_\")[0].split(\"_\")[-1])}\n",
    "\n",
    "# Load train validation data and split it\n",
    "(X_trval_lst,y_trval) = load_data(DPATH,data_type,label_type,True,hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "X_trval_lst = standardize_static(X_trval_lst)\n",
    "\n",
    "# Train model\n",
    "MODDIR = form_train_model(X_trval_lst,y_trval,hyper_dict,label_type,is_tune=False,epoch_num=200)\n",
    "\n",
    "mod_type = \"multilstm_{}\".format(label_type)\n",
    "RESDIR = '{}{}/'.format(RESULTPATH, mod_type)\n",
    "if not os.path.exists(RESDIR): os.makedirs(RESDIR)\n",
    "\n",
    "# Load test data\n",
    "(X_test1_lst,y_test1) = load_data(DPATH,data_type,label_type,False,hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "X_test1_lst = standardize_static(X_test1_lst)\n",
    "X_test1_lst = [X[:,:,np.newaxis] if X.shape[1] == 60 else X for X in X_test1_lst]\n",
    "X_test1_lst = [np.concatenate(X_test1_lst[:-1],2),X_test1_lst[-1]]\n",
    "\n",
    "# Evaluate on the test set\n",
    "load_model_and_test(RESDIR,MODDIR,X_test1_lst,y_test1,data_type,hosp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] Starting load_min_model_helper()\n",
      "[DEBUG] MPATH /homes/gws/hughchen/phase/downstream_prediction/models/multilstm_1hospdata_desat_bool92_5_nodesatlabel_3numlayer_300numnode_RMSpropopt_0.001lr_0.0drop_1000bsize_200epochnum_300000perepochsize/\n",
      "[DEBUG] Loading model from /homes/gws/hughchen/phase/downstream_prediction//results/multilstm_desat_bool92_5_nodesat/hosp1_data/proc[top15]+nonsignal/\n",
      "[DEBUG] auc_lst.mean(): 0.21062793170203722\n",
      "[DEBUG] roc_auc_lst.mean(): 0.8531248926355575\n"
     ]
    }
   ],
   "source": [
    "load_model_and_test(RESDIR,MODDIR,X_test1_lst,y_test1,data_type,hosp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
