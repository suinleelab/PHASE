{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous variable experiment\n",
    "\n",
    "Embedding models trained in a particular source hospital.  Then, assume the target hospital has a subset of the variables available in the source hospital.  In particular, we will reduce the number of available variables one by one according to the feature importances.\n",
    "\n",
    "The aim of the experiment is to compare whether PHASE embeddings (next) consistently outperform conventional approaches with subsets of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[PROGRESS] ************ label_type nibpm60\n",
      "\n",
      "\n",
      "[PROGRESS] ******** hosp_data 0\n",
      "\n",
      "\n",
      "[PROGRESS] ****** data_type raw[top15]+nonsignal\n",
      "[PROGRESS] *** num_feats 1\n",
      "\n",
      "[Progress] data_type raw[top15]+nonsignal\n",
      "[Progress] trainvalX.shape (1837676, 66)\n",
      "[Progress] test1X.shape    (234659, 66)\n",
      "[PROGRESS] *** num_feats 3\n",
      "\n",
      "[Progress] data_type raw[top15]+nonsignal\n",
      "[Progress] trainvalX.shape (1837676, 186)\n",
      "[Progress] test1X.shape    (234659, 186)\n",
      "[PROGRESS] *** num_feats 5\n",
      "\n",
      "[Progress] data_type raw[top15]+nonsignal\n",
      "[Progress] trainvalX.shape (1837676, 306)\n",
      "[Progress] test1X.shape    (234659, 306)\n",
      "[PROGRESS] *** num_feats 7\n",
      "\n",
      "[Progress] data_type raw[top15]+nonsignal\n",
      "[Progress] trainvalX.shape (1837676, 426)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from phase.prediction import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "PATH  = os.path.expanduser(\"~/phase/\")\n",
    "DPATH = PATH+\"downstream_prediction/\"\n",
    "RESULTPATH = PATH+\"/results/\"\n",
    "MODELPATH  = PATH+\"/models/\"\n",
    "\n",
    "lookback = 60\n",
    "DEBUG = False\n",
    "\n",
    "hypox_feat_lst = [\"SAO2\",\"ETCO2\",\"FIO2\",\"PIP\",\"ECGRATE\",\"TV\",\"ETSEVO\",\"RESPRATE\",\n",
    "                  \"NIBPD\",\"ETSEV\",\"PEAK\",\"PEEP\",\"NIBPS\",\"NIBPM\",\"TEMP1\"]\n",
    "\n",
    "hypot_feat_lst = [\"NIBPM\",\"NIBPD\",\"NIBPS\",\"ECGRATE\",\"PIP\",\"ETCO2\",\"ETSEVO\",\"ETSEV\",\n",
    "                  \"SAO2\",\"PEAK\",\"TV\",\"RESPRATE\",\"FIO2\",\"TEMP1\",\"PEEP\"]\n",
    "\n",
    "hypoc_feat_lst = [\"ETCO2\",\"TV\",\"FIO2\",\"PEAK\",\"RESPRATE\",\"ETSEV\",\"PIP\",\"PEEP\",\n",
    "                  \"ETSEVO\",\"TEMP1\",\"ECGRATE\",\"NIBPS\",\"NIBPD\",\"SAO2\",\"NIBPM\"]\n",
    "\n",
    "label_type_lst = [(\"desat_bool92_5_nodesat\",0.02,\"SAO2\",hypox_feat_lst),\n",
    "                  (\"nibpm60\",0.1,\"NIBPM\",hypot_feat_lst), \n",
    "                  (\"etco235\",0.1,\"ETCO2\",hypoc_feat_lst)]\n",
    "\n",
    "for label_type, eta, curr_feat, feat_lst in label_type_lst[1:]:\n",
    "    print(\"\\n\\n[PROGRESS] ************ label_type {}\".format(label_type))\n",
    "    for hosp_data in [0,1]:\n",
    "        print(\"\\n\\n[PROGRESS] ******** hosp_data {}\".format(hosp_data))\n",
    "        data_type_lst = [\"raw[top15]+nonsignal\", \"nextfive_0[top15]+nonsignal\", \"nextfive_1[top15]+nonsignal\"]\n",
    "        \n",
    "        xgb_type = \"xgb_{}_top15_eta{}\".format(label_type,eta)\n",
    "        RESDIR = '{}{}/'.format(RESULTPATH, xgb_type) \n",
    "        if not os.path.exists(RESDIR): os.makedirs(RESDIR)\n",
    "        for data_type in data_type_lst:\n",
    "            print(\"\\n\\n[PROGRESS] ****** data_type {}\".format(data_type))\n",
    "            for num_feats in [1,3,5,7,9,11,13,15]:\n",
    "                print(\"[PROGRESS] *** num_feats {}\".format(num_feats))\n",
    "                # Data type name for training/testing\n",
    "                data_type2 = data_type.replace(\"top15\",\"top\"+str(num_feats))\n",
    "                print(\"\\n[Progress] data_type {}\".format(data_type))\n",
    "\n",
    "                (trainvalX,trainvalY) = load_data(DPATH,data_type,label_type,True,hosp_data,\n",
    "                                                  curr_feat,feat_lst=feat_lst[:num_feats])\n",
    "                print(\"[Progress] trainvalX.shape {}\".format(trainvalX.shape))\n",
    "                if not DEBUG:\n",
    "                    train_xgb_model(RESDIR,trainvalX,trainvalY,data_type2,\n",
    "                                    label_type,hosp_data,eta)\n",
    "\n",
    "                (test1X,test1Y) = load_data(DPATH,data_type,label_type,False,hosp_data,\n",
    "                                            curr_feat,feat_lst=feat_lst[:num_feats])\n",
    "                print(\"[Progress] test1X.shape    {}\".format(test1X.shape))\n",
    "                if not DEBUG:\n",
    "                    load_xgb_model_and_test(RESDIR,test1X,test1Y,data_type2,\n",
    "                                            label_type,hosp_data,xgb_type,eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf36)",
   "language": "python",
   "name": "tf36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
