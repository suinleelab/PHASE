{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream prediction using MLP\n",
    "\n",
    "Perform downstream predictions using different embeddings with an MLP.  Replicate some of the XGB experiments using an MLP downstream model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from matplotlib import cm, pyplot as plt\n",
    "from sklearn import metrics\n",
    "from os.path import expanduser as eu\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import keras\n",
    "import os\n",
    "\n",
    "def load_min_model_helper(MPATH):\n",
    "    print(\"[PROGRESS] Starting load_min_model_helper()\")\n",
    "    print(\"[DEBUG] MPATH {}\".format(MPATH))\n",
    "    mfiles = os.listdir(MPATH)\n",
    "    full_mod_name = MPATH.split(\"/\")[-1]\n",
    "    mfiles = [f for f in mfiles if \"val_loss\" in f]\n",
    "    loss_lst = [float(f.split(\"val_loss:\")[1].split(\"_\")[0]) for f in mfiles]\n",
    "    min_ind = loss_lst.index(min(loss_lst))\n",
    "    min_mod_name = \"{}/{}\".format(MPATH,mfiles[min_ind])\n",
    "    if DEBUG: print(\"[DEBUG] min_mod_name {}\".format(mfiles[min_ind]))\n",
    "    return(load_model(min_mod_name))\n",
    "\n",
    "def train_mlp_model(RESDIR,trainvalX,trainvalY,data_type,label_type,hosp_data):\n",
    "    train_ratio = 0.9\n",
    "    nine_tenths_ind = int(train_ratio*trainvalX.shape[0])\n",
    "    X_train = trainvalX[0:nine_tenths_ind,:]\n",
    "    y_train = trainvalY[0:nine_tenths_ind]\n",
    "    X_valid = trainvalX[nine_tenths_ind:trainvalX.shape[0],:]\n",
    "    y_valid = trainvalY[nine_tenths_ind:trainvalX.shape[0]]\n",
    "    del trainvalX\n",
    "    gc.collect()\n",
    "    # Randomize\n",
    "    indices = np.arange(0,X_train.shape[0])\n",
    "    random.shuffle(indices)\n",
    "    X_train = X_train[indices,:]\n",
    "    y_train = y_train[indices]\n",
    "\n",
    "    indices = np.arange(0,X_valid.shape[0])\n",
    "    random.shuffle(indices)\n",
    "    X_valid = X_valid[indices,:]\n",
    "    y_valid = y_valid[indices]\n",
    "\n",
    "    print(\"[PROGRESS] Starting create_model()\")\n",
    "    # lookback = 60; h1 = 200; h2 = 200;\n",
    "    b_size = 1000; epoch_num = 200; lr = 0.00001\n",
    "    opt_name = \"adam\"\n",
    "    opt = keras.optimizers.Adam(lr)\n",
    "    loss_func = \"binary_crossentropy\"\n",
    "    mod_name = \"multivariate_mlp_label{}_dtype{}_hd{}\".format(label_type,data_type,hosp_data)\n",
    "    mod_name += \"_{}ep_{}ba_{}opt_{}loss\".format(epoch_num,b_size,opt_name,loss_func)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=loss_func, optimizer=opt)\n",
    "\n",
    "    MODDIR = PATH+\"models/\"+mod_name+\"/\"\n",
    "    if not os.path.exists(MODDIR): os.makedirs(MODDIR)\n",
    "\n",
    "    with open(MODDIR+\"loss.txt\", \"w\") as f:\n",
    "        f.write(\"%s\\t%s\\t%s\\t%s\\n\" % (\"i\", \"train_loss\", \"val_loss\", \"epoch_time\"))\n",
    "\n",
    "    # Train and Save\n",
    "    diffs = []; best_loss_so_far = float(\"inf\")\n",
    "    start_time = time.time(); per_iter_size = 300000\n",
    "    for i in range(0,epoch_num):\n",
    "        if per_iter_size < X_train.shape[0]:\n",
    "            per_iter_size = X_train.shape[0]\n",
    "        inds = np.random.choice(X_train.shape[0],per_iter_size,replace=False)\n",
    "        curr_x = X_train[inds,]; curr_y = y_train[inds,]\n",
    "        history = model.fit(curr_x, curr_y, epochs=1, batch_size=1000, \n",
    "                            validation_data=(X_valid,y_valid))\n",
    "\n",
    "        # Save details about training\n",
    "        train_loss = history.history['loss'][0]\n",
    "        val_loss = history.history['val_loss'][0]\n",
    "        epoch_time = time.time() - start_time\n",
    "        with open(MODDIR+\"loss.txt\", \"a\") as f:\n",
    "            f.write(\"%d\\t%f\\t%f\\t%f\\n\" % (i, train_loss, val_loss, epoch_time))\n",
    "\n",
    "        # Save model each iteration\n",
    "        model.save(\"{}val_loss:{}_epoch:{}_{}.h5\".format(MODDIR,val_loss,i,mod_name))\n",
    "    return(MODDIR)\n",
    "\n",
    "def load_mlp_model_and_test(RESDIR,MODDIR,X_test,y_test,data_type,label_type,hosp_data):\n",
    "    model = load_min_model_helper(MODDIR)\n",
    "    save_path = RESDIR+\"hosp{}_data/{}/\".format(hosp_data,data_type)\n",
    "    if not os.path.exists(save_path): os.makedirs(save_path)\n",
    "    print(\"[DEBUG] Loading model from {}\".format(save_path))\n",
    "    ypred = model.predict(X_test)\n",
    "    np.save(save_path+\"ypred.npy\",ypred)\n",
    "    np.save(save_path+\"y_test.npy\",y_test)\n",
    "    auc = metrics.average_precision_score(y_test, ypred)\n",
    "    np.random.seed(231)\n",
    "    auc_lst = []\n",
    "    roc_auc_lst = []\n",
    "    for i in range(0,100):\n",
    "        inds = np.random.choice(X_test.shape[0], X_test.shape[0], replace=True)\n",
    "        auc = metrics.average_precision_score(y_test[inds], ypred[inds])\n",
    "        auc_lst.append(auc)\n",
    "        roc_auc = metrics.roc_auc_score(y_test[inds], ypred[inds])\n",
    "        roc_auc_lst.append(roc_auc)\n",
    "    auc_lst = np.array(auc_lst)\n",
    "    roc_auc_lst = np.array(roc_auc_lst)\n",
    "    print(\"[DEBUG] auc_lst.mean(): {}\".format(auc_lst.mean()))\n",
    "    print(\"[DEBUG] roc_auc_lst.mean(): {}\".format(roc_auc_lst.mean()))\n",
    "\n",
    "    SP = RESDIR+\"hosp{}_data/\".format(hosp_data)\n",
    "    f = open('{}conf_int_hospdata{}_prauc.txt'.format(SP,hosp_data),'a')\n",
    "    f.write(\"{}, {}+-{}\\n\".format(data_type,auc_lst.mean().round(4),2*np.std(auc_lst).round(4)))\n",
    "    f.close()\n",
    "    f = open('{}conf_int_hospdata{}_rocauc.txt'.format(SP,hosp_data),'a')\n",
    "    f.write(\"{}, {}+-{}\\n\".format(data_type,roc_auc_lst.mean().round(4),2*np.std(roc_auc_lst).round(4)))\n",
    "    f.close()\n",
    "    np.save(\"{}auc_lst\".format(save_path,data_type), auc_lst)\n",
    "    np.save(\"{}roc_auc_lst\".format(save_path,data_type), roc_auc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from xgb_setup import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "os.nice(5)\n",
    "PATH = \"/projects/leelab2/hughchen/RELIC/repr_learning/\"\n",
    "DPATH = \"/homes/gws/hughchen/phase/downstream_prediction/\"\n",
    "RESULTPATH = PATH+\"/results/\"; MODELPATH = PATH+\"/models/\"\n",
    "lookback = 60\n",
    "DEBUG = False\n",
    "\n",
    "# label_type_eta_currfeat_lst = [(\"desat_bool92_5_nodesat\",0.02,\"SAO2\"),\n",
    "#                                (\"nibpm60\",0.1,\"NIBPM\"), \n",
    "#                                (\"etco235\",0.1,\"ETCO2\")]\n",
    "\n",
    "# label_type_eta_currfeat_lst = [(\"nibpm60\",0.1,\"NIBPM\"), \n",
    "#                                (\"desat_bool92_5_nodesat\",0.02,\"SAO2\")]\n",
    "label_type_eta_currfeat_lst = [(\"desat_bool92_5_nodesat\",0.02,\"SAO2\")]\n",
    "\n",
    "for label_type, _, curr_feat in label_type_eta_currfeat_lst:\n",
    "    print(\"\\n[Progress] label_type: {}, eta: {}, curr_feat {}\".format(label_type, \"NA\", curr_feat))\n",
    "\n",
    "    xgb_type = \"mlp_{}_top15\".format(label_type)\n",
    "    RESDIR = '{}{}/'.format(RESULTPATH, xgb_type)\n",
    "    if not os.path.exists(RESDIR): os.makedirs(RESDIR)\n",
    "\n",
    "    for hosp_data in [1]:\n",
    "        print(\"\\n[Progress] hosp_data {}\".format(hosp_data))\n",
    "\n",
    "        dt_lst = [\"ema[top15]+nonsignal\",\n",
    "                  \"raw[top15]+nonsignal\",\n",
    "                  \"randemb[top15]+nonsignal\"]\n",
    "        for hosp_model in [0,1,\"P\"]:\n",
    "            if hosp_model == \"P\" and \"desat\" not in label_type: continue\n",
    "                \n",
    "            print(\"\\n[Progress] hosp_model {}\".format(hosp_model))\n",
    "            if hosp_model == 0:\n",
    "                dt_lst += [\"nextfive_{}[top15]+nonsignal\".format(hosp_model),\n",
    "                           \"auto_{}[top15]+nonsignal\".format(hosp_model),\n",
    "                           \"min5_{}[top15]+nonsignal\".format(hosp_model)]\n",
    "            else:\n",
    "                dt_lst += [\"nextfive_{}[top15]+nonsignal\".format(hosp_model),\n",
    "                           \"auto_{}[top15]+nonsignal\".format(hosp_model),\n",
    "                           \"min5_{}[top15]+nonsignal\".format(hosp_model)]\n",
    "\n",
    "            if label_type.startswith(\"desat\"):\n",
    "                dt_lst.append(\"hypox_{}[top15]+nonsignal\".format(hosp_model))\n",
    "            elif label_type.startswith(\"etco2\"):\n",
    "                dt_lst.append(\"hypoc_{}[top15]+nonsignal\".format(hosp_model))\n",
    "            elif label_type.startswith(\"nibpm\"):\n",
    "                dt_lst.append(\"hypot_{}[top15]+nonsignal\".format(hosp_model))\n",
    "\n",
    "        for data_type in dt_lst:\n",
    "            print(\"\\n[Progress] data_type {}\".format(data_type))\n",
    "            (trainvalX,trainvalY) = load_data(DPATH,data_type,label_type,True,\n",
    "                                              hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "            print(\"[Progress] trainvalX.shape {}\".format(trainvalX.shape))\n",
    "            # Standardize data\n",
    "            if \"raw\" in data_type or \"ema\" in data_type:\n",
    "                print(\"[DEBUG] Scaling all features\")\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(trainvalX)\n",
    "                trainvalX = scaler.transform(trainvalX,copy=True)\n",
    "            else:\n",
    "                print(\"[DEBUG] Scaling static features\")\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(trainvalX[:,-6:])\n",
    "                trainvalX[:,-6:] = scaler.transform(trainvalX[:,-6:],copy=True)\n",
    "\n",
    "            if not DEBUG:\n",
    "                MODDIR = train_mlp_model(RESDIR,trainvalX,trainvalY,\n",
    "                                         data_type,label_type,hosp_data)\n",
    "\n",
    "            (test1X,test1Y)       = load_data(DPATH,data_type,label_type,False,\n",
    "                                              hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "            print(\"[Progress] test1X.shape    {}\".format(test1X.shape))\n",
    "            # Standardize data\n",
    "            if \"raw\" in data_type or \"ema\" in data_type:\n",
    "                print(\"[DEBUG] Scaling all features\")\n",
    "                test1X = scaler.transform(test1X,copy=True)\n",
    "            else:\n",
    "                print(\"[DEBUG] Scaling static features\")\n",
    "                test1X[:,-6:] = scaler.transform(test1X[:,-6:],copy=True)\n",
    "\n",
    "            if not DEBUG:\n",
    "                load_mlp_model_and_test(RESDIR,MODDIR,test1X,test1Y,\n",
    "                                        data_type,label_type,hosp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run raw MLPs for nibpm and etco2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Progress] label_type: etco235, eta: NA, curr_feat ETCO2\n",
      "\n",
      "[Progress] hosp_data 1\n",
      "['raw[top15]+nonsignal']\n",
      "\n",
      "[Progress] data_type raw[top15]+nonsignal\n",
      "[DEBUG] Starting load_raw_data\n",
      "[DEBUG] DPATH /homes/gws/hughchen/phase/downstream_prediction//data/etco235/hospital_1/\n",
      "[Progress] trainvalX.shape (1754091, 906)\n",
      "[DEBUG] Scaling all features\n",
      "[PROGRESS] Starting create_model()\n",
      "WARNING:tensorflow:From /homes/gws/hughchen/RNN/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /homes/gws/hughchen/RNN/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /homes/gws/hughchen/RNN/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.4355 - val_loss: 0.2875\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 22s 14us/step - loss: 0.3244 - val_loss: 0.2785\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 21s 13us/step - loss: 0.3060 - val_loss: 0.2754\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 12us/step - loss: 0.2962 - val_loss: 0.2741\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 21s 13us/step - loss: 0.2906 - val_loss: 0.2731\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 12us/step - loss: 0.2866 - val_loss: 0.2729\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2836 - val_loss: 0.2718\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2812 - val_loss: 0.2716\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2790 - val_loss: 0.2708\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 12us/step - loss: 0.2772 - val_loss: 0.2696\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 12us/step - loss: 0.2754 - val_loss: 0.2691\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2741 - val_loss: 0.2685\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 12us/step - loss: 0.2724 - val_loss: 0.2672\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2712 - val_loss: 0.2666\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2698 - val_loss: 0.2655\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 13us/step - loss: 0.2686 - val_loss: 0.2648\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2674 - val_loss: 0.2639\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2664 - val_loss: 0.2627\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2655 - val_loss: 0.2619\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2642 - val_loss: 0.2611\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2634 - val_loss: 0.2602\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2622 - val_loss: 0.2596\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2616 - val_loss: 0.2588\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2609 - val_loss: 0.2586\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 13us/step - loss: 0.2601 - val_loss: 0.2579\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2592 - val_loss: 0.2568\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2583 - val_loss: 0.2560\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2581 - val_loss: 0.2558\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 13us/step - loss: 0.2574 - val_loss: 0.2547\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2566 - val_loss: 0.2541\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2560 - val_loss: 0.2539\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2554 - val_loss: 0.2531\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2548 - val_loss: 0.2524\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2542 - val_loss: 0.2524\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2538 - val_loss: 0.2515\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2531 - val_loss: 0.2516\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2523 - val_loss: 0.2506\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2518 - val_loss: 0.2503\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2516 - val_loss: 0.2500\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2509 - val_loss: 0.2486\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2504 - val_loss: 0.2486\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2499 - val_loss: 0.2482\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2497 - val_loss: 0.2480\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2492 - val_loss: 0.2476\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2489 - val_loss: 0.2469\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2485 - val_loss: 0.2470\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2481 - val_loss: 0.2459\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2477 - val_loss: 0.2459\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2470 - val_loss: 0.2453\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2467 - val_loss: 0.2450\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2462 - val_loss: 0.2445\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2459 - val_loss: 0.2446\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2456 - val_loss: 0.2442\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 13us/step - loss: 0.2454 - val_loss: 0.2436\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2447 - val_loss: 0.2437\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2446 - val_loss: 0.2430\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2442 - val_loss: 0.2429\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2437 - val_loss: 0.2424\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2434 - val_loss: 0.2420\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2430 - val_loss: 0.2416\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2428 - val_loss: 0.2416\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2425 - val_loss: 0.2413\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2422 - val_loss: 0.2410\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2420 - val_loss: 0.2408\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2414 - val_loss: 0.2401\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2411 - val_loss: 0.2400\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2407 - val_loss: 0.2399\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2408 - val_loss: 0.2395\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2401 - val_loss: 0.2392\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2402 - val_loss: 0.2389\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2396 - val_loss: 0.2387\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2397 - val_loss: 0.2385\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2390 - val_loss: 0.2381\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2390 - val_loss: 0.2379\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2386 - val_loss: 0.2377\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2383 - val_loss: 0.2376\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2382 - val_loss: 0.2372\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2376 - val_loss: 0.2369\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2375 - val_loss: 0.2367\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2374 - val_loss: 0.2367\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2371 - val_loss: 0.2365\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2370 - val_loss: 0.2362\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2365 - val_loss: 0.2360\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2365 - val_loss: 0.2359\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2362 - val_loss: 0.2355\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2361 - val_loss: 0.2353\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2356 - val_loss: 0.2352\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2352 - val_loss: 0.2350\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2353 - val_loss: 0.2349\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2350 - val_loss: 0.2349\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2349 - val_loss: 0.2346\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2346 - val_loss: 0.2342\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2344 - val_loss: 0.2342\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2343 - val_loss: 0.2341\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2340 - val_loss: 0.2338\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2338 - val_loss: 0.2336\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2338 - val_loss: 0.2335\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2336 - val_loss: 0.2334\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2332 - val_loss: 0.2333\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 13us/step - loss: 0.2332 - val_loss: 0.2332\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2328 - val_loss: 0.2328\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2329 - val_loss: 0.2331\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2326 - val_loss: 0.2327\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2323 - val_loss: 0.2324\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2323 - val_loss: 0.2325\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 13us/step - loss: 0.2321 - val_loss: 0.2324\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2318 - val_loss: 0.2322\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2316 - val_loss: 0.2321\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2316 - val_loss: 0.2319\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2313 - val_loss: 0.2317\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2311 - val_loss: 0.2315\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2312 - val_loss: 0.2315\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2308 - val_loss: 0.2315\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2306 - val_loss: 0.2312\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2306 - val_loss: 0.2312\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2305 - val_loss: 0.2312\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2303 - val_loss: 0.2309\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2300 - val_loss: 0.2309\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2299 - val_loss: 0.2308\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2300 - val_loss: 0.2305\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2297 - val_loss: 0.2304\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2295 - val_loss: 0.2304\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2294 - val_loss: 0.2304\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2292 - val_loss: 0.2301\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2292 - val_loss: 0.2301\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2291 - val_loss: 0.2299\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2288 - val_loss: 0.2299\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2288 - val_loss: 0.2299\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 12us/step - loss: 0.2285 - val_loss: 0.2297\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2286 - val_loss: 0.2298\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2283 - val_loss: 0.2294\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2283 - val_loss: 0.2294\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2280 - val_loss: 0.2295\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2281 - val_loss: 0.2294\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2279 - val_loss: 0.2291\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2279 - val_loss: 0.2291\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2276 - val_loss: 0.2291\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2275 - val_loss: 0.2291\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2274 - val_loss: 0.2289\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2272 - val_loss: 0.2286\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2273 - val_loss: 0.2288\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2271 - val_loss: 0.2287\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2270 - val_loss: 0.2285\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2268 - val_loss: 0.2286\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2266 - val_loss: 0.2285\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2267 - val_loss: 0.2284\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2265 - val_loss: 0.2283\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 12us/step - loss: 0.2264 - val_loss: 0.2283\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2263 - val_loss: 0.2282\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2260 - val_loss: 0.2282\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2261 - val_loss: 0.2280\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2260 - val_loss: 0.2280\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2261 - val_loss: 0.2278\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2260 - val_loss: 0.2278\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2256 - val_loss: 0.2277\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2255 - val_loss: 0.2278\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2253 - val_loss: 0.2277\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2254 - val_loss: 0.2274\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2253 - val_loss: 0.2275\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2251 - val_loss: 0.2275\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2251 - val_loss: 0.2275\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2249 - val_loss: 0.2274\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2249 - val_loss: 0.2274\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2250 - val_loss: 0.2272\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2247 - val_loss: 0.2270\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2247 - val_loss: 0.2271\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2246 - val_loss: 0.2269\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2245 - val_loss: 0.2272\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2243 - val_loss: 0.2269\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2245 - val_loss: 0.2268\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2243 - val_loss: 0.2267\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2242 - val_loss: 0.2268\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2242 - val_loss: 0.2268\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2240 - val_loss: 0.2269\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2239 - val_loss: 0.2267\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2237 - val_loss: 0.2265\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 13us/step - loss: 0.2235 - val_loss: 0.2264\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2236 - val_loss: 0.2265\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2235 - val_loss: 0.2264\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2233 - val_loss: 0.2265\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2234 - val_loss: 0.2263\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2230 - val_loss: 0.2265\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 20s 13us/step - loss: 0.2230 - val_loss: 0.2264\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2230 - val_loss: 0.2262\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2231 - val_loss: 0.2262\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2230 - val_loss: 0.2260\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2229 - val_loss: 0.2263\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2227 - val_loss: 0.2259\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2226 - val_loss: 0.2261\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2227 - val_loss: 0.2259\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2226 - val_loss: 0.2260\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2225 - val_loss: 0.2258\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2225 - val_loss: 0.2256\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2224 - val_loss: 0.2256\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2224 - val_loss: 0.2256\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2222 - val_loss: 0.2258\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2222 - val_loss: 0.2257\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2220 - val_loss: 0.2258\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2221 - val_loss: 0.2255\n",
      "Train on 1578681 samples, validate on 175410 samples\n",
      "Epoch 1/1\n",
      "1578681/1578681 [==============================] - 19s 12us/step - loss: 0.2220 - val_loss: 0.2255\n",
      "[DEBUG] Starting load_raw_data\n",
      "[DEBUG] DPATH /homes/gws/hughchen/phase/downstream_prediction//data/etco235/hospital_1/\n",
      "[Progress] test1X.shape    (232126, 906)\n",
      "[DEBUG] Scaling all features\n",
      "[PROGRESS] Starting load_min_model_helper()\n",
      "[DEBUG] MPATH /projects/leelab2/hughchen/RELIC/repr_learning/models/multivariate_mlp_labeletco235_dtyperaw[top15]+nonsignal_hd1_200ep_1000ba_adamopt_binary_crossentropyloss/\n",
      "[DEBUG] Loading model from /projects/leelab2/hughchen/RELIC/repr_learning//results/mlp_etco235_top15/hosp1_data/raw[top15]+nonsignal/\n",
      "[DEBUG] auc_lst.mean(): 0.383185691969\n",
      "[DEBUG] roc_auc_lst.mean(): 0.82910741937\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from xgb_setup import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "os.nice(5)\n",
    "PATH = \"/projects/leelab2/hughchen/RELIC/repr_learning/\"\n",
    "DPATH = \"/homes/gws/hughchen/phase/downstream_prediction/\"\n",
    "RESULTPATH = PATH+\"/results/\"; MODELPATH = PATH+\"/models/\"\n",
    "lookback = 60\n",
    "DEBUG = False\n",
    "\n",
    "# label_type_eta_currfeat_lst = [(\"desat_bool92_5_nodesat\",0.02,\"SAO2\"),\n",
    "#                                (\"nibpm60\",0.1,\"NIBPM\"), \n",
    "#                                (\"etco235\",0.1,\"ETCO2\")]\n",
    "\n",
    "label_type_eta_currfeat_lst = [(\"etco235\",0.1,\"ETCO2\")]\n",
    "\n",
    "for label_type, _, curr_feat in label_type_eta_currfeat_lst:\n",
    "    print(\"\\n[Progress] label_type: {}, eta: {}, curr_feat {}\".format(label_type, \"NA\", curr_feat))\n",
    "\n",
    "    xgb_type = \"mlp_{}_top15\".format(label_type)\n",
    "    RESDIR = '{}{}/'.format(RESULTPATH, xgb_type)\n",
    "    if not os.path.exists(RESDIR): os.makedirs(RESDIR)\n",
    "\n",
    "    for hosp_data in [1]:\n",
    "        print(\"\\n[Progress] hosp_data {}\".format(hosp_data))\n",
    "\n",
    "        dt_lst = [\"raw[top15]+nonsignal\"]\n",
    "        print(dt_lst)\n",
    "        for data_type in dt_lst:\n",
    "            print(\"\\n[Progress] data_type {}\".format(data_type))\n",
    "            (trainvalX,trainvalY) = load_data(DPATH,data_type,label_type,True,\n",
    "                                              hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "            print(\"[Progress] trainvalX.shape {}\".format(trainvalX.shape))\n",
    "            # Standardize data\n",
    "            if \"raw\" in data_type or \"ema\" in data_type:\n",
    "                print(\"[DEBUG] Scaling all features\")\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(trainvalX)\n",
    "                trainvalX = scaler.transform(trainvalX,copy=True)\n",
    "            else:\n",
    "                print(\"[DEBUG] Scaling static features\")\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(trainvalX[:,-6:])\n",
    "                trainvalX[:,-6:] = scaler.transform(trainvalX[:,-6:],copy=True)\n",
    "\n",
    "            if not DEBUG:\n",
    "                MODDIR = train_mlp_model(RESDIR,trainvalX,trainvalY,\n",
    "                                         data_type,label_type,hosp_data)\n",
    "\n",
    "            (test1X,test1Y)       = load_data(DPATH,data_type,label_type,False,\n",
    "                                              hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "            print(\"[Progress] test1X.shape    {}\".format(test1X.shape))\n",
    "            # Standardize data\n",
    "            if \"raw\" in data_type or \"ema\" in data_type:\n",
    "                print(\"[DEBUG] Scaling all features\")\n",
    "                test1X = scaler.transform(test1X,copy=True)\n",
    "            else:\n",
    "                print(\"[DEBUG] Scaling static features\")\n",
    "                test1X[:,-6:] = scaler.transform(test1X[:,-6:],copy=True)\n",
    "\n",
    "            if not DEBUG:\n",
    "                load_mlp_model_and_test(RESDIR,MODDIR,test1X,test1Y,\n",
    "                                        data_type,label_type,hosp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf36)",
   "language": "python",
   "name": "tf36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
