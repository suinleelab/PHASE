{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream prediction using MLP\n",
    "\n",
    "Perform downstream predictions using different embeddings with an MLP.  Replicate some of the XGB experiments using an MLP downstream model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from phase.prediction import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "PATH  = os.path.expanduser(\"~/phase/\")\n",
    "DPATH = PATH+\"downstream_prediction/\"\n",
    "RESULTPATH = PATH+\"/results/\"\n",
    "MODELPATH  = PATH+\"/models/\"\n",
    "\n",
    "lookback = 60\n",
    "DEBUG = False\n",
    "\n",
    "label_type_eta_currfeat_lst = [(\"desat_bool92_5_nodesat\",0.02,\"SAO2\"),\n",
    "                               (\"nibpm60\",0.1,\"NIBPM\"), \n",
    "                               (\"etco235\",0.1,\"ETCO2\")]\n",
    "\n",
    "for label_type, _, curr_feat in label_type_eta_currfeat_lst:\n",
    "    print(\"\\n[Progress] label_type: {}, eta: {}, curr_feat {}\".format(label_type, \"NA\", curr_feat))\n",
    "\n",
    "    xgb_type = \"mlp_{}_top15\".format(label_type)\n",
    "    RESDIR = '{}{}/'.format(RESULTPATH, xgb_type)\n",
    "    if not os.path.exists(RESDIR): os.makedirs(RESDIR)\n",
    "\n",
    "    for hosp_data in [1]:\n",
    "        print(\"\\n[Progress] hosp_data {}\".format(hosp_data))\n",
    "\n",
    "        dt_lst = [\"ema[top15]+nonsignal\",\n",
    "                  \"raw[top15]+nonsignal\",\n",
    "                  \"randemb[top15]+nonsignal\"]\n",
    "        for hosp_model in [0,1,\"P\"]:\n",
    "            if hosp_model == \"P\" and \"desat\" not in label_type: continue\n",
    "                \n",
    "            print(\"\\n[Progress] hosp_model {}\".format(hosp_model))\n",
    "            if hosp_model == 0:\n",
    "                dt_lst += [\"nextfive_{}[top15]+nonsignal\".format(hosp_model),\n",
    "                           \"auto_{}[top15]+nonsignal\".format(hosp_model),\n",
    "                           \"min5_{}[top15]+nonsignal\".format(hosp_model)]\n",
    "            else:\n",
    "                dt_lst += [\"nextfive_{}[top15]+nonsignal\".format(hosp_model),\n",
    "                           \"auto_{}[top15]+nonsignal\".format(hosp_model),\n",
    "                           \"min5_{}[top15]+nonsignal\".format(hosp_model)]\n",
    "\n",
    "            if label_type.startswith(\"desat\"):\n",
    "                dt_lst.append(\"hypox_{}[top15]+nonsignal\".format(hosp_model))\n",
    "            elif label_type.startswith(\"etco2\"):\n",
    "                dt_lst.append(\"hypoc_{}[top15]+nonsignal\".format(hosp_model))\n",
    "            elif label_type.startswith(\"nibpm\"):\n",
    "                dt_lst.append(\"hypot_{}[top15]+nonsignal\".format(hosp_model))\n",
    "\n",
    "        for data_type in dt_lst:\n",
    "            print(\"\\n[Progress] data_type {}\".format(data_type))\n",
    "            (trainvalX,trainvalY) = load_data(DPATH,data_type,label_type,True,\n",
    "                                              hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "            print(\"[Progress] trainvalX.shape {}\".format(trainvalX.shape))\n",
    "            # Standardize data\n",
    "            if \"raw\" in data_type or \"ema\" in data_type:\n",
    "                print(\"[DEBUG] Scaling all features\")\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(trainvalX)\n",
    "                trainvalX = scaler.transform(trainvalX,copy=True)\n",
    "            else:\n",
    "                print(\"[DEBUG] Scaling static features\")\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(trainvalX[:,-6:])\n",
    "                trainvalX[:,-6:] = scaler.transform(trainvalX[:,-6:],copy=True)\n",
    "\n",
    "            if not DEBUG:\n",
    "                MODDIR = train_mlp_model(RESDIR,trainvalX,trainvalY,\n",
    "                                         data_type,label_type,hosp_data)\n",
    "\n",
    "            (test1X,test1Y)       = load_data(DPATH,data_type,label_type,False,\n",
    "                                              hosp_data,curr_feat,DEBUG=DEBUG)\n",
    "            print(\"[Progress] test1X.shape    {}\".format(test1X.shape))\n",
    "            # Standardize data\n",
    "            if \"raw\" in data_type or \"ema\" in data_type:\n",
    "                print(\"[DEBUG] Scaling all features\")\n",
    "                test1X = scaler.transform(test1X,copy=True)\n",
    "            else:\n",
    "                print(\"[DEBUG] Scaling static features\")\n",
    "                test1X[:,-6:] = scaler.transform(test1X[:,-6:],copy=True)\n",
    "\n",
    "            if not DEBUG:\n",
    "                load_mlp_model_and_test(RESDIR,MODDIR,test1X,test1Y,\n",
    "                                        data_type,label_type,hosp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf36)",
   "language": "python",
   "name": "tf36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
